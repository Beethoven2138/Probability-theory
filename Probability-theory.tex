\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{cite}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{exercise}[theorem]{Exercise}


\title{Probability Theory}
\author{Saxon Supple}
\date{May 2025}

\begin{document}

\maketitle

\begin{exercise}
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, and let $A_1,...,A_n$ be measurable sets. Prove by induction that\[\mathbb{P}(\bigcup_{i=1}^nA_i)\geq\sum_{i=1}^n\mathbb{P}(A_i)-\sum_{1\leq i < j\leq n}\mathbb{P}(A_i\cap A_j).\]
\end{exercise}
\begin{proof}
For $n=1$, \[\mathbb{P}\left(\bigcup_{i=1}^n A_i\right)=\mathbb{P}(A_1)\geq\sum_{i=1}^n\mathbb{P}(A_i)-\sum_{1\leq i < j\leq n}\mathbb{P}(A_i\cap A_j)=\mathbb{P}(A_1).\] Now assume true for $n=k$. Then for $n=k+1$ we have\begin{align*}
\mathbb{P}\left(\bigcup_{i=1}^{k+1}A_i\right)&=\mathbb{P}\left(A_{k+1}\cup\bigcup_{i=1}^{k}A_i\right)\\&=\mathbb{P}(A_{k+1})+\mathbb{P}\left(\bigcup_{i=1}^{k}A_i\right)-\mathbb{P}\left(\bigcup_{i=1}^{k}(A_i\cap A_{k+1})\right)\\&\geq\mathbb{P}(A_{k+1})+\sum_{i=1}^k\mathbb{P}(A_i)-\sum_{1\leq i<j\leq k}\mathbb{P}(A_i\cap A_j)-\mathbb{P}\left(\bigcup_{i=1}^{k}(A_i\cap A_{k+1})\right)\\&\geq\sum_{i=1}^{k+1}\mathbb{P}(A_i)-\sum_{1\leq i<j\leq k}\mathbb{P}(A_i\cap A_j)-\sum_{i=1}^k\mathbb{P}(A_i\cap A_{k+1})\\&=\sum_{i=1}^{k+1}\mathbb{P}(A_i)-\sum_{1\leq i<j\leq k+1}\mathbb{P}(A_i\cap A_j).
\end{align*}
Hence the result holds by induction.
\end{proof}

\begin{exercise}
Suppose $(\Omega,\mathcal{A},\mathbb{P})$ is a probability space and $X:\Omega\to[0,\infty)$ is a non-negative random variable.
\begin{enumerate}
    \item[(a)] Let $A_n=\{\omega\in\Omega:X(\omega)\leq n\}$. Prove that $\mathbb{E}[X\mathbf{1}_{A_n
    }]\to\mathbb{E}[X]$ as $n\to\infty$.
    \item[(b)] Prove that if $\mathbb{E}[X]<\infty$, then $\mathbb{E}[X\mathbf{1}_{A_n^c}]\to0$ as $n\to\infty$.
    \item[(c)] Suppose $\mathbb{E}[X]<\infty$. Prove that\[\lim_{\delta\downarrow0}\sup_{\{A\in\mathcal{F}:\mathbb{P}(A)<\delta\}}\mathbb{E}[X\mathbf{1}_A]=0.\]
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $0\leq X\mathbf{1}_{A_n}\uparrow X$ so apply the monotone convergence theorem.
    \item[(b)] $\mathbb{E}[X\mathbf{1}_{A_n}]+\mathbb{E}[X\mathbf{1}_{A_n^c}]=\mathbb{E}[X]\forall n$ so $\lim_{n\to\infty}\mathbb{E}[X\mathbf{1}_{A_n^c}]=\mathbb{E}[X]-\lim_{n\to\infty}\mathbb{E}[X\mathbf{1}_{A_n}]=0$.
    \item[(c)] Let $(\delta_n)_{n\in\mathbb{N}}$ be a sequence such that $\delta_n\downarrow0$. We want to show that given an $\epsilon > 0$ there exists an $N\in\mathbb{N}$ such that $\forall n>N$ we have\[\sup_{\{A\in\mathcal{F}:\mathbb{P}(A)<\delta_n\}}\mathbb{E}[X\mathbf{1}_A]<\epsilon.\] To do so, we instead show that there exists an $N\in\mathbb{N}$ such that $\forall n>N$ we have\[\mathbb{E}[X\mathbf{1}_A]<\frac{\epsilon}{2}\forall A\in\mathcal{F}:\mathbb{P}(A)<\delta_n,\] which would imply that \[\sup_{\{A\in\mathcal{F}:\mathbb{P}(A)<\delta_n\}}\mathbb{E}[X\mathbf{1}_A]\leq\frac{\epsilon}{2}<\epsilon.\] By part (b) there exists an $M\in\mathbb{N}$ such that \[\mathbb{E}[X\mathbf{1}_{\{ X>M\}}]<\frac{\epsilon}{4}.\] Let $N\in\mathbb{N}$ be such that $\delta_n<\frac{\epsilon}{4M}\forall n>N$. Then if $A\in\mathcal{F}$ is such that $\mathbb{P}(A)<\delta_n$, we have that\[X\mathbf{1}_A=X\mathbf{1}_{A\cap \{X>M\}}+X\mathbf{1}_{A\cap \{X\leq M\}}\]so\begin{align*}\mathbb{E}[X\mathbf{1}_A]&\leq \mathbb{E}[X\mathbf{1}_{\{X>M\}}]+\mathbb{E}[X\mathbf{1}_{A\cap\{X\leq M\}}]\\&<\frac{\epsilon}{4}+M\mathbb{P}(A)\\&<\frac{\epsilon}{4}+\frac{M\epsilon}{4M}=\frac{\epsilon}{2},\end{align*} as required.
\end{enumerate}
\end{proof}

\begin{exercise}
Let $\Omega=\{1,2,3,4,5,6\}$ with $\mathcal{A}=\mathcal{P}(\Omega)$ and let $\mathcal{C}=\{A,B\}$ with $A=\{1,2,3,4\}$ and $B=\{3,4,5\}$. What are the sets of $\sigma(\mathcal{C})$?
\end{exercise}
\begin{proof}
\[A\cap B=\{3,4\},A^c\cap B=\{5\},A\cap B^c=\{1,2\},A^c\cap B^c=\{6\}.\]
Hence, if we label the sets as \[X_1=\{3,4\},X_2=\{5\},X_3=\{1,2\},X_4=\{6\}\] then\[\sigma(\mathcal{C})=\{\bigcup_{i\in J}X_i:J\subseteq\{1,2,3,4\}\}.\]
\end{proof}

\begin{exercise}
Let $\Omega=\{1,2,3,4\}$ and let $\mathcal{A}$ be the $\sigma$-algebra $\sigma(\{\{1\},\{2\},\{3,4\}\})$ on $\Omega$.
\begin{enumerate}
    \item[(a)] Give an example of a subset of $\Omega$ that is not in $\mathcal{A}$.
    \item[(b)] Give an example of a function $f:\Omega\to\mathbb{R}$ that is not measurable.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $\{1,4\}$.
    \item[(b)] $\mathbf{1}_{\{1,4\}}$.
\end{enumerate}
\end{proof}

\begin{exercise}
Let $(\Omega,\mathcal{A},\mathbb{P})$ be a probability space. Suppose that $\mathcal{C}$ is a finite partition of $\Omega$ with $\mathcal{C}\subset\mathcal{A}$, i.e. suppose that $\mathcal{C}=\{A_1,...,A_n\}$ with the sets $A_i$ partitioning $\Omega$, and with each $A_i$ in $\mathcal{A}$. Likewise, suppose that $\mathcal{B}=\{B_1,...,B_m\}$ is a finite partition of $\Omega$ with $\mathcal{B}\subset\mathcal{A}$.
\begin{enumerate}
    \item[(a)] Suppose that $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)$ for all events $A\in\mathcal{C}$ and $B\in\mathcal{B}$. Prove that $\sigma(\mathcal{C})$ and $\sigma(\mathcal{B})$ are independent $\sigma$-algebras.
    \item[(b)] Does the conclusion still hold if $\mathcal{C}=\{A_1,A_2,...\}$ and $\mathcal{B}=\{B_1,B_2,...\}$ are countably infinite partitions of $\Omega$ but otherwise things are as described above? Explain briefly.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] Let $A\in\sigma(\mathcal{C})$ and $B\in\sigma(\mathcal{B})$. Then $A=\bigcup_{i\in I}A_i$ and $B=\bigcup_{j\in J}B_j$ for some \[I\subseteq\{1,...,n\},J\subseteq\{1,...,m\}.\] Hence \begin{align*}\mathbb{P}(A\cap B)&=\mathbb{P}\left(\bigcup_{i\in I}A_i\cap\bigcup_{j\in J}B_j\right)\\&=\mathbb{P}\left(\bigcup_{i\in I}\left(A_i\cap \bigcup_{j\in J}B_j\right)\right)\\&=\sum_{i\in I}\mathbb{P}\left(A_i\cap\bigcup_{j\in J}B_j\right)\\&=\sum_{i\in I}\mathbb{P}\left(\bigcup_{j\in J}(A_i\cap B_j)\right)\\&=\sum_{i\in I}\sum_{j\in J}\mathbb{P}(A_i\cap B_j)\\&=\sum_{i\in I}\sum_{j\in J}\mathbb{P}(A_i)\mathbb{P}(B_j)\\&=\left(\sum_{i\in I}\mathbb{P}(A_i)\right)\left(\sum_{j\in J}\mathbb{P}(B_j)\right)\\&=\mathbb{P}(A)\mathbb{P}(B).\end{align*}
    \item[(b)] Yes, since the argument still holds with countably infinite indexes.
\end{enumerate}
\end{proof}

\begin{exercise}
\begin{enumerate}
    \item[(a)] Prove Markov's inequality, which says that if $X$  is a non-negative random variable and $a>0$, then $\mathbb{P}(X\geq a)\leq a^{-1}\mathbb{E}[X]$.
    \item[(b)] Prove that, if $X$ is a non-negative random variable with $\mathbb{E}[X]=0$, then $\mathbb{P}(X=0)=1$.
    \item[(c)] Suppose $X$ is a random variable with $\mathbb{E}[X]=\mu$ and $\text{Var}(X)=0$. Prove that $\mathbb{P}(X=\mu)=1$.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] \begin{align*}
        \mathbb{P}(X\geq a)&=\mathbb{E}[\mathbf{1}_{X\geq a}]\\&\leq\mathbb{E}\left[\frac{X}{a}\mathbf{1}_{X\geq a}\right]\\&\leq a^{-1}\mathbb{E}[X].
    \end{align*}
    \item[(b)] Let $\epsilon > 0$. Then $\mathbb{P}(X\geq\epsilon)\leq\epsilon^{-1}\cdot\mathbb{E}[X]=0$. $\epsilon$ is arbitrary so $\mathbb{P}(X>0)=\mathbb{P}(X\neq 0)=0$. Hence $\mathbb{P}(X=0)=1$.
    \item[(c)] $\text{Var}(X)=\mathbb{E}[(X-\mu)^2]=0$ so $\mathbb{P}((X-\mu)^2=0)=\mathbb{P}(X=\mu)=1$.
\end{enumerate}
\end{proof}

\begin{exercise}
Let $X$ and $Y$ be random variables on the same probability space $(\Omega,\mathcal{F},\mathbb{P})$.
\begin{enumerate}
    \item[(a)] Let $\mathcal{C}$ be the class of events of the form $\{X<t\},t\in\mathbb{R}$. Show that $\sigma(\mathcal{C})=\sigma(X)$.
    \item[(b)] Show that $\mathcal{C}$ is a $\pi$-system.
    \item[(c)] Suppose that $\mathbb{P}(\{X<a\}\cap\{Y<b\})=\mathbb{P}(X<a)\mathbb{P}(Y<b)$ for all real numbers $a$ and $b$. Let $\mathcal{C}$ be as in (a), and $\mathcal{C}'=\{\{Y<t\}:t\in\mathbb{R}\}$.
    \begin{itemize}
        \item[1.] Fix $A\in\mathcal{C}$ and set $\mu_A(B)=\mathbb{P}(A\cap B)$ and $\mu_A'(B)=\mathbb{P}(A)\mathbb{P}(B)$ for all $B\in\sigma(\mathcal{C}')$. Show that $\mu_A=\mu_A'$ on $\sigma(\mathcal{C}')$.
        \item[2.] Fix $B\in\sigma(\mathcal{C}')$, and set $\nu_B(A)=\mathbb{P}(A\cap B)$ and $\nu_B'(A)=\mathbb{P}(A)\mathbb{P}(B)$ for all $A\in\sigma(\mathcal{C})$. Show that $\nu_B=\nu_B'$ on $\sigma(\mathcal{C})$.
        \item[3.] Show that $X$ and $Y$ are independent random variables.
    \end{itemize}
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $(-\infty,t)$ is a Borel set $\forall t$ so $\mathcal{C}\subseteq\sigma(X)$, and hence $\sigma(\mathcal{C})\subseteq\sigma(X)$. $\mathcal{B}=\sigma(\mathcal{I})$, where $\mathcal{I}:=\{(-\infty,t):t\in\mathbb{R}\}$. Let $\mathcal{M}:=\{A\in\mathcal{B}:X^{-1}(A)\in\sigma(\mathcal{C})\}$. Clearly $\mathcal{I}\subseteq\mathcal{M}$. Let $A_1,A_2,...\in\mathcal{M}$. Then $X^{-1}(\bigcup_{i=1}^\infty A_i)=\bigcup_{i=1}^\infty X^{-1}(A_i)\in\sigma(\mathcal{C})$, since $\sigma(\mathcal{C})$ is a $\sigma$-algebra. Hence $\bigcup_{i=1}^\infty A_i\in\mathcal{M}$. Furthermore, $X^{-1}(\emptyset)=\emptyset\in\sigma(\mathcal{C})$ so $\emptyset\in\mathcal{M}$. Finally, Let $A\in\mathcal{M}$. Then $X^{-1}(A^c)=X^{-1}(A)^c\in\sigma(\mathcal{C})$ so $A^c\in\mathcal{M}$. Hence $\mathcal{M}$ is a $\sigma$-algebra so contains $\sigma(\mathcal{I})=\mathcal{B}$; that is, $X^{-1}(A)\in\sigma(\mathcal{C})\forall A\in\mathcal{B}$, so $\sigma(X)\subseteq\sigma(\mathcal{C})$. Hence $\sigma(X)=\sigma(\mathcal{C})$.
    \item[(b)] $\{X<t\}\cap\{X<s\}=\{X<\min(t,s)\}\in\mathcal{C}$.
    \item[(c)]
    \begin{itemize}
        \item[1.] $\mu_A$ and $\mu'_A$ agree on $\mathcal{C}'$, and $\mathcal{C}'$ is a $\pi$-system, so $\mu_A$ and $\mu'_A$ agree on $\sigma(\mathcal{C}')$ by the uniqueness lemma.
        \item[2.] $\nu_B$ and $\nu'_B$ agree on $\mathcal{C}$ by the above, and $\mathcal{C}$ is a $\pi$-system, so $\nu_B$ and $\nu'_B$ agree on $\sigma(\mathcal{C})$ by the uniqueness lemma.
        \item[3.] We have shown that $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)\forall A\in\sigma(\mathcal{C}),B\in\sigma(\mathcal{C}')$, and hence \[\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)\forall A\in\sigma(X),B\in\sigma(Y),\] since $\sigma(\mathcal{C})=\sigma(X),\sigma(\mathcal{C}')=\sigma(Y)$. Thus $X$ and $Y$ are independent.
    \end{itemize}
\end{enumerate}
\end{proof}

\begin{exercise}
Let $\mathcal{F}$ be a sub-$\sigma$-algebra. Let $U$ and $V$ be two $\mathcal{F}$-measurable random variables.
\begin{enumerate}
    \item[(a)] Show that $U+V$ is $\mathcal{F}$-measurable.
    \item[(b)] Show that $UV$ is $\mathcal{F}$-measurable.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $\{U+V<t\}=\bigcup_{s\in\mathbb{R}}\{U<s\}\cap\{V<t-s\}$. Furthermore, if $U(\omega)<s$ and $V(\omega)<t-s$ for some $s\in\mathbb{R}$, by the density of $\mathbb{Q}$ in $\mathbb{R}$ there will be some $q\in\mathbb{Q}$ such that $U(\omega)<q<s$ and hence $V(\omega)<t-s<t-q$, so $\omega\in\{U<q\}\cap\{V<t-q\}$. This is true for all $\omega\in\{U+V<t\}$ so $\{U+V<t\}\subseteq\bigcup_{q\in\mathbb{Q}}\{U<q\}\cap\{V<t-q\}$. The reverse inclusion clearly holds so \[\{U+V<t\}=\bigcup_{q\in\mathbb{Q}}\{U<q\}\cap\{V<t-q\}\in\mathcal{F}.\] Hence $U+V$ is $\mathcal{F}$-measurable.
    \item[(b)] Let $X$ be an $\mathcal{F}$-measurable random variable. Then $\{X^2<t\}=\{-\sqrt{t}<X<\sqrt{t}\}\in\mathcal{F}$ if $t\geq 0$, and $\{X^2<t\}=\emptyset\in\mathcal{F}$ otherwise. Hence $X^2$ is $\mathcal{F}$-measurable.
    Hence $UV=\frac{(U+V)^2-(U-V)^2}{4}$ is $\mathcal{F}$-measurable.
\end{enumerate}
\end{proof}
\begin{exercise}
Let $(X_i)_{i \geq 1}$ be a sequence of independent random variables such that $\mathbb{E}[|X_i|] < \infty$ for all $i \geq 1$. For all $n \geq 1$, set $Y_n = \prod_{i=1}^n X_i$.

\begin{enumerate}
    \item[(a)] Prove that, for all $n \geq 1$, $Y_n = \prod_{i=1}^n X_i$ is $\sigma(X_1, \ldots, X_n)$-measurable, and deduce that $\sigma(Y_n) \subset \sigma(X_1, \ldots, X_n)$.
    
    \item[(b)] Fix $n \geq 1$ and let
    \[
    \mathcal{C} = \left\{ \bigcap_{i=1}^n \{X_i \in B_i\} : B_1, \ldots, B_n \in \mathcal{B}(\mathbb{R}) \right\}.
    \]
    Show that $\mathcal{C}$ is a $\pi$-system.
    
    \item[(c)] Show that, for all $A \in \sigma(X_{n+1})$ and $C \in \mathcal{C}$, $\mathbb{P}(A \cap C) = \mathbb{P}(A)\mathbb{P}(C)$.
    
    \item[(d)] Conclude that, for all $A \in \sigma(X_{n+1})$ and $C \in \sigma(X_1, \ldots, X_n)$, $\mathbb{P}(A \cap C) = \mathbb{P}(A)\mathbb{P}(C)$.
    
    \item[(e)] Deduce that, for all $n \geq 1$, $Y_n$ and $X_{n+1}$ are independent.
    
    \item[(f)] By induction on $n$, prove that, for all $n \geq 1$,
    \[
    \mathbb{E} \left[ \prod_{i=1}^n X_i \right] = \prod_{i=1}^n \mathbb{E}[X_i]. \tag{2}
    \]
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $\sigma(X_i)\subseteq\sigma(X_1,...,X_n)\forall i$ so each $X_i$ is $\sigma(X_1,...,X_n)$-measurable. The product of measurable functions is measurable so $Y_n$ is $\sigma(X_1,...,X_n)$-measurable. It clearly follows that $\sigma(Y_n)\subseteq\sigma(X_1,...,X_n)$.
    \item[(b)] Let $A_1,...,A_n,B_1,...,B_n\in\mathcal{B}$. Then\[\bigcap_{i=1}^n\{X_i\in A_i\}\cap\bigcap_{i=1}^n\{X_i\in B_i\}=\bigcap_{i=1}^n\{X_i\in A_i\cap B_i\}\in\mathcal{C}.\]
    \item[(c)] Let $A=\{X_{n+1}\in B_{n+1}\}$ and let $C=\bigcap_{i=1}^n\{X_i\in B_i\}$ for some $B_1,...,B_{n+1}\in\mathcal{B}$. Then by independence we have \begin{align*}\mathbb{P}(A\cap C)&=\mathbb{P}(\bigcap_{i=1}^{n+1}\{X_i\in B_i\})\\&=\prod_{i=1}^{n+1}\mathbb{P}(X_i\in B_i)\\&=\mathbb{P}(A)\prod_{i=1}^n\mathbb{P}(X_i\in B_i)\\&=\mathbb{P}(A)\mathbb{P}(\bigcap_{i=1}^n\{X_i\in B_i\})\\&=\mathbb{P}(A)\mathbb{P}(C).\end{align*}
    \item[(d)] First note that $\sigma(X_i)\subseteq\mathcal{C}\forall i\in\{1,..,n\}$ so $\sigma(X_1)\cup...\cup\sigma(X_n)\subseteq\mathcal{C}$, and hence $\sigma(X_1,...,X_n)\subseteq\sigma(\mathcal{C})$, and furthermore that $\mathcal{C}\subseteq\sigma(X_1,...,X_n)$ so $\sigma(\mathcal{C})=\sigma(X_1,...,X_n)$. $\mathcal{C}$ is a $\pi$-system so apply the uniqueness lemma to the measures $\nu_A:\sigma(X_1,...,X_n)\to\mathbb{R}:C\mapsto\mathbb{P}(A\cap C)$ and $\nu_A':\sigma(X_1,...,X_n)\to\mathbb{R}:C\mapsto\mathbb{P}(A)\mathbb{P}(C)$ for every $A\in\sigma(X_{n+1})$.
    \item[(e)] Let $A\in\sigma(X_{n+1})$ and $C\in\sigma(Y_n)$. $\sigma(Y_n)\subseteq\sigma(X_1,...,X_n)$ so $\mathbb{P}(A\cap C)=\mathbb{P}(A)\mathbb{P}(C)$. Hence $Y_n$ and $X_{n+1}$ are independent.
    \item[(f)] The base case is clear. Assume true for $n=k$:\[\mathbb{E}\left[\prod_{i=1}^k X_i\right]=\prod_{i=1}^k\mathbb{E}[X_i].\] For $n=k+1$, we have that $X_{k+1}$ and $\prod_{i=1}^k X_i$ are independent so\[\mathbb{E}\left[\prod_{i=1}^{k+1}X_i\right]=\mathbb{E}\left[X_{k+1}\prod_{i=1}^kX_i\right]=\mathbb{E}[X_{k+1}]\mathbb{E}\left[\prod_{i=1}^kX_i\right]=\mathbb{E}[X_{k+1}]\prod_{i=1}^k\mathbb{E}[X_i]=\prod_{i=1}^{k+1}\mathbb{E}[X_i].\]
\end{enumerate}
\end{proof}
\begin{exercise}
Let $p \in (0,1)$. Let $X$ and $Y$ be two independent random variables on a probability space $(\Omega, \mathcal{A}, \mathbb{P})$, such that
\[
\mathbb{P}(X = 1) = \mathbb{P}(Y = 1) = p \quad \text{and} \quad \mathbb{P}(X = -1) = \mathbb{P}(Y = -1) = 1 - p.
\]

\begin{enumerate}
    \item[(a)] Let $A \in \mathcal{A}$ be an event. Give the formula for $\mathbb{E}[X \mid \sigma(A)]$ in terms of $\mathbb{E}[X \mid A]$ and $\mathbb{E}[X \mid A^c]$.
    
    \item[(b)] Show that $\mathbb{E}[X \mid X + Y = 0] = 0$.
    
    \item[(c)] Show that
    \[
    \mathbb{E}[X \mid X + Y \neq 0] = \frac{2p - 1}{p^2 + (1 - p)^2}.
    \]
    
    \item[(d)] Let $\mathcal{F}$ be the sub-$\sigma$-algebra generated by the event $\{X + Y = 0\}$. Calculate $\mathbb{E}[X \mid \mathcal{F}]$.
    
    \item[(e)] Calculate $\mathbb{E}[Y \mid \mathcal{F}]$. Are the random variables $\mathbb{E}[X \mid \mathcal{F}]$ and $\mathbb{E}[Y \mid \mathcal{F}]$ independent?\\
    \textit{(Hint: be aware that the answer might depend on the value of $p$.)}
\end{enumerate}
\end{exercise}
\begin{exercise}
Let $X$ and $Y$ be two random variables such that $\mathbb{E}[|X|] < \infty$ and $\mathbb{E}[|Y|] < \infty$. Assume that
\[
\mathbb{E}[X \mid Y] = Y \text{ a.s. and } \mathbb{E}[Y \mid X] = X \text{ a.s.}
\]

\begin{enumerate}
    \item[(a)] Prove that, for all $c \in \mathbb{Q}$, $\mathbb{E}[(X - Y) \mathbf{1}_{Y \leq c}] = 0$.
    
    \item[(b)] Deduce that, for all $c \in \mathbb{Q}$, $\mathbb{E}[(X - Y) \mathbf{1}_{\{X \leq c\} \cap \{Y \leq c\}}] \leq 0$.
    
    \item[(c)] Using the symmetry of $X$ and $Y$, deduce that, for all $c \in \mathbb{Q}$, 
    \[
    \mathbb{E}[(X - Y) \mathbf{1}_{\{X \leq c\} \cap \{Y \leq c\}}] = 0.
    \]
    
    \item[(d)] Deduce that, for all $c \in \mathbb{Q}$, $\mathbb{P}(X > c \text{ and } Y \leq c) = 0$.
    
    \item[(e)] Deduce that $X = Y$ almost surely.
\end{enumerate}
\end{exercise}
\begin{exercise}
Assume that we toss $n$ fair coins and $N$ of them come up as heads. Suppose we then roll $N$ fair dice. Let $T$ be the sum of the values obtained on the $N$ dice. Express $\mathbb{E}[T|N]$ as a function of $N$, and then express $\mathbb{E}[T]$ and $\mathbb{E}[NT]$, both as functions of $n$.
\end{exercise}
\end{document}
