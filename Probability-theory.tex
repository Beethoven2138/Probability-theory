\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{cite}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{exercise}[theorem]{Exercise}


\title{Probability Theory}
\author{Saxon Supple}
\date{May 2025}

\begin{document}

\maketitle

\[\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\] so \[\mathbb{P}(A|B)=\frac{1}{\mathbb{P}(B)}\int_A\mathbf{1}_Bd\mathbb{P}.\] Hence,\[\mathbb{E}[X|B]=\int Xd\mathbb{P}(\cdot |B)=\frac{1}{\mathbb{P}(B)}\int X\mathbf{1}_Bd\mathbb{P}=\frac{\mathbb{E}[X\mathbf{1}_B]}{\mathbb{P}(B)}.\]


Let $X$ be a non-negative integer-valued random variable. Then \[\mathbb{E}[X]=\sum_{i=1}^\infty\mathbb{P}(X\geq i).\]


\begin{exercise}
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, and let $A_1,...,A_n$ be measurable sets. Prove by induction that\[\mathbb{P}(\bigcup_{i=1}^nA_i)\geq\sum_{i=1}^n\mathbb{P}(A_i)-\sum_{1\leq i < j\leq n}\mathbb{P}(A_i\cap A_j).\]
\end{exercise}
\begin{proof}
For $n=1$, \[\mathbb{P}\left(\bigcup_{i=1}^n A_i\right)=\mathbb{P}(A_1)\geq\sum_{i=1}^n\mathbb{P}(A_i)-\sum_{1\leq i < j\leq n}\mathbb{P}(A_i\cap A_j)=\mathbb{P}(A_1).\] Now assume true for $n=k$. Then for $n=k+1$ we have\begin{align*}
\mathbb{P}\left(\bigcup_{i=1}^{k+1}A_i\right)&=\mathbb{P}\left(A_{k+1}\cup\bigcup_{i=1}^{k}A_i\right)\\&=\mathbb{P}(A_{k+1})+\mathbb{P}\left(\bigcup_{i=1}^{k}A_i\right)-\mathbb{P}\left(\bigcup_{i=1}^{k}(A_i\cap A_{k+1})\right)\\&\geq\mathbb{P}(A_{k+1})+\sum_{i=1}^k\mathbb{P}(A_i)-\sum_{1\leq i<j\leq k}\mathbb{P}(A_i\cap A_j)-\mathbb{P}\left(\bigcup_{i=1}^{k}(A_i\cap A_{k+1})\right)\\&\geq\sum_{i=1}^{k+1}\mathbb{P}(A_i)-\sum_{1\leq i<j\leq k}\mathbb{P}(A_i\cap A_j)-\sum_{i=1}^k\mathbb{P}(A_i\cap A_{k+1})\\&=\sum_{i=1}^{k+1}\mathbb{P}(A_i)-\sum_{1\leq i<j\leq k+1}\mathbb{P}(A_i\cap A_j).
\end{align*}
Hence the result holds by induction.
\end{proof}

\begin{exercise}
Suppose $(\Omega,\mathcal{A},\mathbb{P})$ is a probability space and $X:\Omega\to[0,\infty)$ is a non-negative random variable.
\begin{enumerate}
    \item[(a)] Let $A_n=\{\omega\in\Omega:X(\omega)\leq n\}$. Prove that $\mathbb{E}[X\mathbf{1}_{A_n
    }]\to\mathbb{E}[X]$ as $n\to\infty$.
    \item[(b)] Prove that if $\mathbb{E}[X]<\infty$, then $\mathbb{E}[X\mathbf{1}_{A_n^c}]\to0$ as $n\to\infty$.
    \item[(c)] Suppose $\mathbb{E}[X]<\infty$. Prove that\[\lim_{\delta\downarrow0}\sup_{\{A\in\mathcal{F}:\mathbb{P}(A)<\delta\}}\mathbb{E}[X\mathbf{1}_A]=0.\]
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $0\leq X\mathbf{1}_{A_n}\uparrow X$ so apply the monotone convergence theorem.
    \item[(b)] $\mathbb{E}[X\mathbf{1}_{A_n}]+\mathbb{E}[X\mathbf{1}_{A_n^c}]=\mathbb{E}[X]\forall n$ so $\lim_{n\to\infty}\mathbb{E}[X\mathbf{1}_{A_n^c}]=\mathbb{E}[X]-\lim_{n\to\infty}\mathbb{E}[X\mathbf{1}_{A_n}]=0$.
    \item[(c)] Let $(\delta_n)_{n\in\mathbb{N}}$ be a sequence such that $\delta_n\downarrow0$. We want to show that given an $\epsilon > 0$ there exists an $N\in\mathbb{N}$ such that $\forall n>N$ we have\[\sup_{\{A\in\mathcal{F}:\mathbb{P}(A)<\delta_n\}}\mathbb{E}[X\mathbf{1}_A]<\epsilon.\] To do so, we instead show that there exists an $N\in\mathbb{N}$ such that $\forall n>N$ we have\[\mathbb{E}[X\mathbf{1}_A]<\frac{\epsilon}{2}\forall A\in\mathcal{F}:\mathbb{P}(A)<\delta_n,\] which would imply that \[\sup_{\{A\in\mathcal{F}:\mathbb{P}(A)<\delta_n\}}\mathbb{E}[X\mathbf{1}_A]\leq\frac{\epsilon}{2}<\epsilon.\] By part (b) there exists an $M\in\mathbb{N}$ such that \[\mathbb{E}[X\mathbf{1}_{\{ X>M\}}]<\frac{\epsilon}{4}.\] Let $N\in\mathbb{N}$ be such that $\delta_n<\frac{\epsilon}{4M}\forall n>N$. Then if $A\in\mathcal{F}$ is such that $\mathbb{P}(A)<\delta_n$, we have that\[X\mathbf{1}_A=X\mathbf{1}_{A\cap \{X>M\}}+X\mathbf{1}_{A\cap \{X\leq M\}}\]so\begin{align*}\mathbb{E}[X\mathbf{1}_A]&\leq \mathbb{E}[X\mathbf{1}_{\{X>M\}}]+\mathbb{E}[X\mathbf{1}_{A\cap\{X\leq M\}}]\\&<\frac{\epsilon}{4}+M\mathbb{P}(A)\\&<\frac{\epsilon}{4}+\frac{M\epsilon}{4M}=\frac{\epsilon}{2},\end{align*} as required.
\end{enumerate}
\end{proof}

\begin{exercise}
Let $\Omega=\{1,2,3,4,5,6\}$ with $\mathcal{A}=\mathcal{P}(\Omega)$ and let $\mathcal{C}=\{A,B\}$ with $A=\{1,2,3,4\}$ and $B=\{3,4,5\}$. What are the sets of $\sigma(\mathcal{C})$?
\end{exercise}
\begin{proof}
\[A\cap B=\{3,4\},A^c\cap B=\{5\},A\cap B^c=\{1,2\},A^c\cap B^c=\{6\}.\]
Hence, if we label the sets as \[X_1=\{3,4\},X_2=\{5\},X_3=\{1,2\},X_4=\{6\}\] then\[\sigma(\mathcal{C})=\{\bigcup_{i\in J}X_i:J\subseteq\{1,2,3,4\}\}.\]
\end{proof}

\begin{exercise}
Let $\Omega=\{1,2,3,4\}$ and let $\mathcal{A}$ be the $\sigma$-algebra $\sigma(\{\{1\},\{2\},\{3,4\}\})$ on $\Omega$.
\begin{enumerate}
    \item[(a)] Give an example of a subset of $\Omega$ that is not in $\mathcal{A}$.
    \item[(b)] Give an example of a function $f:\Omega\to\mathbb{R}$ that is not measurable.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $\{1,4\}$.
    \item[(b)] $\mathbf{1}_{\{1,4\}}$.
\end{enumerate}
\end{proof}

\begin{exercise}
Let $(\Omega,\mathcal{A},\mathbb{P})$ be a probability space. Suppose that $\mathcal{C}$ is a finite partition of $\Omega$ with $\mathcal{C}\subset\mathcal{A}$, i.e. suppose that $\mathcal{C}=\{A_1,...,A_n\}$ with the sets $A_i$ partitioning $\Omega$, and with each $A_i$ in $\mathcal{A}$. Likewise, suppose that $\mathcal{B}=\{B_1,...,B_m\}$ is a finite partition of $\Omega$ with $\mathcal{B}\subset\mathcal{A}$.
\begin{enumerate}
    \item[(a)] Suppose that $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)$ for all events $A\in\mathcal{C}$ and $B\in\mathcal{B}$. Prove that $\sigma(\mathcal{C})$ and $\sigma(\mathcal{B})$ are independent $\sigma$-algebras.
    \item[(b)] Does the conclusion still hold if $\mathcal{C}=\{A_1,A_2,...\}$ and $\mathcal{B}=\{B_1,B_2,...\}$ are countably infinite partitions of $\Omega$ but otherwise things are as described above? Explain briefly.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] Let $A\in\sigma(\mathcal{C})$ and $B\in\sigma(\mathcal{B})$. Then $A=\bigcup_{i\in I}A_i$ and $B=\bigcup_{j\in J}B_j$ for some \[I\subseteq\{1,...,n\},J\subseteq\{1,...,m\}.\] Hence \begin{align*}\mathbb{P}(A\cap B)&=\mathbb{P}\left(\bigcup_{i\in I}A_i\cap\bigcup_{j\in J}B_j\right)\\&=\mathbb{P}\left(\bigcup_{i\in I}\left(A_i\cap \bigcup_{j\in J}B_j\right)\right)\\&=\sum_{i\in I}\mathbb{P}\left(A_i\cap\bigcup_{j\in J}B_j\right)\\&=\sum_{i\in I}\mathbb{P}\left(\bigcup_{j\in J}(A_i\cap B_j)\right)\\&=\sum_{i\in I}\sum_{j\in J}\mathbb{P}(A_i\cap B_j)\\&=\sum_{i\in I}\sum_{j\in J}\mathbb{P}(A_i)\mathbb{P}(B_j)\\&=\left(\sum_{i\in I}\mathbb{P}(A_i)\right)\left(\sum_{j\in J}\mathbb{P}(B_j)\right)\\&=\mathbb{P}(A)\mathbb{P}(B).\end{align*}
    \item[(b)] Yes, since the argument still holds with countably infinite indexes.
\end{enumerate}
\end{proof}

\begin{exercise}
\begin{enumerate}
    \item[(a)] Prove Markov's inequality, which says that if $X$  is a non-negative random variable and $a>0$, then $\mathbb{P}(X\geq a)\leq a^{-1}\mathbb{E}[X]$.
    \item[(b)] Prove that, if $X$ is a non-negative random variable with $\mathbb{E}[X]=0$, then $\mathbb{P}(X=0)=1$.
    \item[(c)] Suppose $X$ is a random variable with $\mathbb{E}[X]=\mu$ and $\text{Var}(X)=0$. Prove that $\mathbb{P}(X=\mu)=1$.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] \begin{align*}
        \mathbb{P}(X\geq a)&=\mathbb{E}[\mathbf{1}_{X\geq a}]\\&\leq\mathbb{E}\left[\frac{X}{a}\mathbf{1}_{X\geq a}\right]\\&\leq a^{-1}\mathbb{E}[X].
    \end{align*}
    \item[(b)] Let $\epsilon > 0$. Then $\mathbb{P}(X\geq\epsilon)\leq\epsilon^{-1}\cdot\mathbb{E}[X]=0$. $\epsilon$ is arbitrary so $\mathbb{P}(X>0)=\mathbb{P}(X\neq 0)=0$. Hence $\mathbb{P}(X=0)=1$.
    \item[(c)] $\text{Var}(X)=\mathbb{E}[(X-\mu)^2]=0$ so $\mathbb{P}((X-\mu)^2=0)=\mathbb{P}(X=\mu)=1$.
\end{enumerate}
\end{proof}

\begin{exercise}
Let $X$ and $Y$ be random variables on the same probability space $(\Omega,\mathcal{F},\mathbb{P})$.
\begin{enumerate}
    \item[(a)] Let $\mathcal{C}$ be the class of events of the form $\{X<t\},t\in\mathbb{R}$. Show that $\sigma(\mathcal{C})=\sigma(X)$.
    \item[(b)] Show that $\mathcal{C}$ is a $\pi$-system.
    \item[(c)] Suppose that $\mathbb{P}(\{X<a\}\cap\{Y<b\})=\mathbb{P}(X<a)\mathbb{P}(Y<b)$ for all real numbers $a$ and $b$. Let $\mathcal{C}$ be as in (a), and $\mathcal{C}'=\{\{Y<t\}:t\in\mathbb{R}\}$.
    \begin{itemize}
        \item[1.] Fix $A\in\mathcal{C}$ and set $\mu_A(B)=\mathbb{P}(A\cap B)$ and $\mu_A'(B)=\mathbb{P}(A)\mathbb{P}(B)$ for all $B\in\sigma(\mathcal{C}')$. Show that $\mu_A=\mu_A'$ on $\sigma(\mathcal{C}')$.
        \item[2.] Fix $B\in\sigma(\mathcal{C}')$, and set $\nu_B(A)=\mathbb{P}(A\cap B)$ and $\nu_B'(A)=\mathbb{P}(A)\mathbb{P}(B)$ for all $A\in\sigma(\mathcal{C})$. Show that $\nu_B=\nu_B'$ on $\sigma(\mathcal{C})$.
        \item[3.] Show that $X$ and $Y$ are independent random variables.
    \end{itemize}
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $(-\infty,t)$ is a Borel set $\forall t$ so $\mathcal{C}\subseteq\sigma(X)$, and hence $\sigma(\mathcal{C})\subseteq\sigma(X)$. $\mathcal{B}=\sigma(\mathcal{I})$, where $\mathcal{I}:=\{(-\infty,t):t\in\mathbb{R}\}$. Let $\mathcal{M}:=\{A\in\mathcal{B}:X^{-1}(A)\in\sigma(\mathcal{C})\}$. Clearly $\mathcal{I}\subseteq\mathcal{M}$. Let $A_1,A_2,...\in\mathcal{M}$. Then $X^{-1}(\bigcup_{i=1}^\infty A_i)=\bigcup_{i=1}^\infty X^{-1}(A_i)\in\sigma(\mathcal{C})$, since $\sigma(\mathcal{C})$ is a $\sigma$-algebra. Hence $\bigcup_{i=1}^\infty A_i\in\mathcal{M}$. Furthermore, $X^{-1}(\emptyset)=\emptyset\in\sigma(\mathcal{C})$ so $\emptyset\in\mathcal{M}$. Finally, Let $A\in\mathcal{M}$. Then $X^{-1}(A^c)=X^{-1}(A)^c\in\sigma(\mathcal{C})$ so $A^c\in\mathcal{M}$. Hence $\mathcal{M}$ is a $\sigma$-algebra so contains $\sigma(\mathcal{I})=\mathcal{B}$; that is, $X^{-1}(A)\in\sigma(\mathcal{C})\forall A\in\mathcal{B}$, so $\sigma(X)\subseteq\sigma(\mathcal{C})$. Hence $\sigma(X)=\sigma(\mathcal{C})$.
    \item[(b)] $\{X<t\}\cap\{X<s\}=\{X<\min(t,s)\}\in\mathcal{C}$.
    \item[(c)]
    \begin{itemize}
        \item[1.] $\mu_A$ and $\mu'_A$ agree on $\mathcal{C}'$, and $\mathcal{C}'$ is a $\pi$-system, so $\mu_A$ and $\mu'_A$ agree on $\sigma(\mathcal{C}')$ by the uniqueness lemma.
        \item[2.] $\nu_B$ and $\nu'_B$ agree on $\mathcal{C}$ by the above, and $\mathcal{C}$ is a $\pi$-system, so $\nu_B$ and $\nu'_B$ agree on $\sigma(\mathcal{C})$ by the uniqueness lemma.
        \item[3.] We have shown that $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)\forall A\in\sigma(\mathcal{C}),B\in\sigma(\mathcal{C}')$, and hence \[\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)\forall A\in\sigma(X),B\in\sigma(Y),\] since $\sigma(\mathcal{C})=\sigma(X),\sigma(\mathcal{C}')=\sigma(Y)$. Thus $X$ and $Y$ are independent.
    \end{itemize}
\end{enumerate}
\end{proof}

\begin{exercise}
Let $\mathcal{F}$ be a sub-$\sigma$-algebra. Let $U$ and $V$ be two $\mathcal{F}$-measurable random variables.
\begin{enumerate}
    \item[(a)] Show that $U+V$ is $\mathcal{F}$-measurable.
    \item[(b)] Show that $UV$ is $\mathcal{F}$-measurable.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $\{U+V<t\}=\bigcup_{s\in\mathbb{R}}\{U<s\}\cap\{V<t-s\}$. Furthermore, if $U(\omega)<s$ and $V(\omega)<t-s$ for some $s\in\mathbb{R}$, by the density of $\mathbb{Q}$ in $\mathbb{R}$ there will be some $q\in\mathbb{Q}$ such that $U(\omega)<q<s$ and hence $V(\omega)<t-s<t-q$, so $\omega\in\{U<q\}\cap\{V<t-q\}$. This is true for all $\omega\in\{U+V<t\}$ so $\{U+V<t\}\subseteq\bigcup_{q\in\mathbb{Q}}\{U<q\}\cap\{V<t-q\}$. The reverse inclusion clearly holds so \[\{U+V<t\}=\bigcup_{q\in\mathbb{Q}}\{U<q\}\cap\{V<t-q\}\in\mathcal{F}.\] Hence $U+V$ is $\mathcal{F}$-measurable.
    \item[(b)] Let $X$ be an $\mathcal{F}$-measurable random variable. Then $\{X^2<t\}=\{-\sqrt{t}<X<\sqrt{t}\}\in\mathcal{F}$ if $t\geq 0$, and $\{X^2<t\}=\emptyset\in\mathcal{F}$ otherwise. Hence $X^2$ is $\mathcal{F}$-measurable.
    Hence $UV=\frac{(U+V)^2-(U-V)^2}{4}$ is $\mathcal{F}$-measurable.
\end{enumerate}
\end{proof}
\begin{exercise}
Let $(X_i)_{i \geq 1}$ be a sequence of independent random variables such that $\mathbb{E}[|X_i|] < \infty$ for all $i \geq 1$. For all $n \geq 1$, set $Y_n = \prod_{i=1}^n X_i$.

\begin{enumerate}
    \item[(a)] Prove that, for all $n \geq 1$, $Y_n = \prod_{i=1}^n X_i$ is $\sigma(X_1, \ldots, X_n)$-measurable, and deduce that $\sigma(Y_n) \subset \sigma(X_1, \ldots, X_n)$.
    
    \item[(b)] Fix $n \geq 1$ and let
    \[
    \mathcal{C} = \left\{ \bigcap_{i=1}^n \{X_i \in B_i\} : B_1, \ldots, B_n \in \mathcal{B}(\mathbb{R}) \right\}.
    \]
    Show that $\mathcal{C}$ is a $\pi$-system.
    
    \item[(c)] Show that, for all $A \in \sigma(X_{n+1})$ and $C \in \mathcal{C}$, $\mathbb{P}(A \cap C) = \mathbb{P}(A)\mathbb{P}(C)$.
    
    \item[(d)] Conclude that, for all $A \in \sigma(X_{n+1})$ and $C \in \sigma(X_1, \ldots, X_n)$, $\mathbb{P}(A \cap C) = \mathbb{P}(A)\mathbb{P}(C)$.
    
    \item[(e)] Deduce that, for all $n \geq 1$, $Y_n$ and $X_{n+1}$ are independent.
    
    \item[(f)] By induction on $n$, prove that, for all $n \geq 1$,
    \[
    \mathbb{E} \left[ \prod_{i=1}^n X_i \right] = \prod_{i=1}^n \mathbb{E}[X_i]. \tag{2}
    \]
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $\sigma(X_i)\subseteq\sigma(X_1,...,X_n)\forall i$ so each $X_i$ is $\sigma(X_1,...,X_n)$-measurable. The product of measurable functions is measurable so $Y_n$ is $\sigma(X_1,...,X_n)$-measurable. It clearly follows that $\sigma(Y_n)\subseteq\sigma(X_1,...,X_n)$.
    \item[(b)] Let $A_1,...,A_n,B_1,...,B_n\in\mathcal{B}$. Then\[\bigcap_{i=1}^n\{X_i\in A_i\}\cap\bigcap_{i=1}^n\{X_i\in B_i\}=\bigcap_{i=1}^n\{X_i\in A_i\cap B_i\}\in\mathcal{C}.\]
    \item[(c)] Let $A=\{X_{n+1}\in B_{n+1}\}$ and let $C=\bigcap_{i=1}^n\{X_i\in B_i\}$ for some $B_1,...,B_{n+1}\in\mathcal{B}$. Then by independence we have \begin{align*}\mathbb{P}(A\cap C)&=\mathbb{P}(\bigcap_{i=1}^{n+1}\{X_i\in B_i\})\\&=\prod_{i=1}^{n+1}\mathbb{P}(X_i\in B_i)\\&=\mathbb{P}(A)\prod_{i=1}^n\mathbb{P}(X_i\in B_i)\\&=\mathbb{P}(A)\mathbb{P}(\bigcap_{i=1}^n\{X_i\in B_i\})\\&=\mathbb{P}(A)\mathbb{P}(C).\end{align*}
    \item[(d)] First note that $\sigma(X_i)\subseteq\mathcal{C}\forall i\in\{1,..,n\}$ so $\sigma(X_1)\cup...\cup\sigma(X_n)\subseteq\mathcal{C}$, and hence $\sigma(X_1,...,X_n)\subseteq\sigma(\mathcal{C})$, and furthermore that $\mathcal{C}\subseteq\sigma(X_1,...,X_n)$ so $\sigma(\mathcal{C})=\sigma(X_1,...,X_n)$. $\mathcal{C}$ is a $\pi$-system so apply the uniqueness lemma to the measures $\nu_A:\sigma(X_1,...,X_n)\to\mathbb{R}:C\mapsto\mathbb{P}(A\cap C)$ and $\nu_A':\sigma(X_1,...,X_n)\to\mathbb{R}:C\mapsto\mathbb{P}(A)\mathbb{P}(C)$ for every $A\in\sigma(X_{n+1})$.
    \item[(e)] Let $A\in\sigma(X_{n+1})$ and $C\in\sigma(Y_n)$. $\sigma(Y_n)\subseteq\sigma(X_1,...,X_n)$ so $\mathbb{P}(A\cap C)=\mathbb{P}(A)\mathbb{P}(C)$. Hence $Y_n$ and $X_{n+1}$ are independent.
    \item[(f)] The base case is clear. Assume true for $n=k$:\[\mathbb{E}\left[\prod_{i=1}^k X_i\right]=\prod_{i=1}^k\mathbb{E}[X_i].\] For $n=k+1$, we have that $X_{k+1}$ and $\prod_{i=1}^k X_i$ are independent so\[\mathbb{E}\left[\prod_{i=1}^{k+1}X_i\right]=\mathbb{E}\left[X_{k+1}\prod_{i=1}^kX_i\right]=\mathbb{E}[X_{k+1}]\mathbb{E}\left[\prod_{i=1}^kX_i\right]=\mathbb{E}[X_{k+1}]\prod_{i=1}^k\mathbb{E}[X_i]=\prod_{i=1}^{k+1}\mathbb{E}[X_i].\]
\end{enumerate}
\end{proof}
\begin{exercise}
Let $p \in (0,1)$. Let $X$ and $Y$ be two independent random variables on a probability space $(\Omega, \mathcal{A}, \mathbb{P})$, such that
\[
\mathbb{P}(X = 1) = \mathbb{P}(Y = 1) = p \quad \text{and} \quad \mathbb{P}(X = -1) = \mathbb{P}(Y = -1) = 1 - p.
\]

\begin{enumerate}
    \item[(a)] Let $A \in \mathcal{A}$ be an event. Give the formula for $\mathbb{E}[X \mid \sigma(A)]$ in terms of $\mathbb{E}[X \mid A]$ and $\mathbb{E}[X \mid A^c]$.
    
    \item[(b)] Show that $\mathbb{E}[X \mid X + Y = 0] = 0$.
    
    \item[(c)] Show that
    \[
    \mathbb{E}[X \mid X + Y \neq 0] = \frac{2p - 1}{p^2 + (1 - p)^2}.
    \]
    
    \item[(d)] Let $\mathcal{F}$ be the sub-$\sigma$-algebra generated by the event $\{X + Y = 0\}$. Calculate $\mathbb{E}[X \mid \mathcal{F}]$.
    
    \item[(e)] Calculate $\mathbb{E}[Y \mid \mathcal{F}]$. Are the random variables $\mathbb{E}[X \mid \mathcal{F}]$ and $\mathbb{E}[Y \mid \mathcal{F}]$ independent?\\
    \textit{(Hint: be aware that the answer might depend on the value of $p$.)}
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] $\sigma(A)=\{A,A^c\}$. Hence \[\mathbb{E}[X|\sigma(A)]=\mathbb{E}[X|A]\mathbf{1}_A+\mathbb{E}[X|A^c]\mathbf{1}_{A^c}\] almost surely. To see this, first note that $\mathbb{E}[X|A]\mathbf{1}_A+\mathbb{E}[X|A^c]\mathbf{1}_{A^c}$ is $\sigma(A)$-measurable, and furthermore \begin{align*}\mathbb{E}[(\mathbb{E}[X|A]\mathbf{1}_A+\mathbb{E}[X|A^c]\mathbf{1}_{A^c})\mathbf{1}_A]&=\mathbb{E}[\mathbb{E}[X|A]\mathbf{1}_A]\\&=\mathbb{E}\left[\frac{\mathbb{E}[X\mathbf{1}_A]}{\mathbb{P}(A)}\mathbf{1}_A\right]\\&=\mathbb{E}[X\mathbf{1}_A],\end{align*} and similarly \[\mathbb{E}[(\mathbb{E}[X|A]\mathbf{1}_A+\mathbb{E}[X|A^c]\mathbf{1}_{A^c})\mathbf{1}_{A^c}]=\mathbb{E}[X\mathbf{1}_{A^c}].\]
    \item[(b)] \[\mathbb{E}[X|X+Y=0]=\frac{\mathbb{E}[X\mathbf{1}_{X+Y=0}]}{\mathbb{P}(X+Y=0)}.\]Since \[\{X+Y=0\}=\{X=1,Y=-1\}\sqcup\{X=-1,Y=1\}\] it follows that\begin{align*}
    \mathbb{E}[X\mathbf{1}_{X+Y=0}]&=\mathbb{P}(X=1,Y=-1)-\mathbb{P}(X=-1,Y=1)\\&=p(1-p)-(1-p)p=0.
    \end{align*}Hence\[\mathbb{E}[X|X+Y=0]=0.\]
    \item[(c)] \[\mathbb{E}[X|X+Y\neq0]=\frac{\mathbb{E}[X\mathbf{1}_{X+Y\neq0}]}{\mathbb{P}(X+Y\neq0)}.\] Since\[\{X+Y\neq0\}=\{X=1,Y=1\}\sqcup\{X=-1,Y=-1\},\] it follows that\begin{align*}
    \mathbb{E}[X\mathbf{1}_{X+Y\neq0}]&=\mathbb{P}(X=1,Y=1)-\mathbb{P}(X=-1,Y=-1)\\&=p^2-(1-p)^2\\&=p^2-1+2p-p^2=2p-1
    \end{align*}and\begin{align*}
    \mathbb{P}(X+Y\neq0)&=\mathbb{P}(X=1,Y=1)+\mathbb{P}(X=-1,Y=-1)\\&=p^2+(1-p)^2.
    \end{align*}
    Hence\[\mathbb{E}[X|X+Y\neq0]=\frac{2p-1}{p^2+(1-p)^2}.\]
    \item[(d)] \[\mathbb{E}[X|\mathcal{F}]=\frac{2p-1}{p^2+(1-p)^2}\mathbf{1}_{X+Y\neq0}.\]
    \item[(e)] \[\mathbb{E}[Y|\mathcal{F}]=\mathbb{E}[X|\mathcal{F}]\] by symmetry.
    First suppose that $\frac{2p-1}{p^2+(1-p)^2}\neq 0$.\[\mathbb{P}\left(\mathbb{E}[X|\mathcal{F}]=\frac{2p-1}{p^2+(1-p)^2},\mathbb{E}[Y|\mathcal{F}]=\frac{2p-1}{p^2+(1-p)^2}\right)=\mathbb{P}(X+Y\neq0)=p^2+(1-p)^2.\]and\[\mathbb{P}\left(\mathbb{E}[X|\mathcal{F}]=\frac{2p-1}{p^2+(1-p)^2}\right)\mathbb{P}\left(\mathbb{E}[Y|\mathcal{F}]=\frac{2p-1}{p^2+(1-p)^2}\right)=\mathbb{P}(X+Y\neq0)^2=(p^2+(1-p)^2)^2.\] Hence in order to have independence we need\begin{align*}&p^2+(1-p)^2=(p^2+(1-p)^2)^2\\\iff &p^2+(1-p)^2=1\\\iff &p^2-p=0\\\iff &p\in\{0,1\}.\end{align*}
    Now suppose $\frac{2p-1}{p^2+(1-p)^2}=0$. Then $\mathbb{E}[Y|\mathcal{F}]=\mathbb{E}[X|\mathcal{F}]=0$ almost surely so are independent. The only value of $p$ which gives this is $p=\frac{1}{2}$.
\end{enumerate}
\end{proof}

\begin{exercise}
Let $X$ and $Y$ be two random variables such that $\mathbb{E}[|X|] < \infty$ and $\mathbb{E}[|Y|] < \infty$. Assume that
\[
\mathbb{E}[X \mid Y] = Y \text{ a.s. and } \mathbb{E}[Y \mid X] = X \text{ a.s.}
\]

\begin{enumerate}
    \item[(a)] Prove that, for all $c \in \mathbb{Q}$, $\mathbb{E}[(X - Y) \mathbf{1}_{Y \leq c}] = 0$.
    
    \item[(b)] Deduce that, for all $c \in \mathbb{Q}$, $\mathbb{E}[(X - Y) \mathbf{1}_{\{X \leq c\} \cap \{Y \leq c\}}] \leq 0$.
    
    \item[(c)] Using the symmetry of $X$ and $Y$, deduce that, for all $c \in \mathbb{Q}$, 
    \[
    \mathbb{E}[(X - Y) \mathbf{1}_{\{X \leq c\} \cap \{Y \leq c\}}] = 0.
    \]
    
    \item[(d)] Deduce that, for all $c \in \mathbb{Q}$, $\mathbb{P}(X > c \text{ and } Y \leq c) = 0$.
    
    \item[(e)] Deduce that $X = Y$ almost surely.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] \[\mathbb{E}[(X-Y)\mathbf{1}_{Y\leq c}]=\mathbb{E}[X\mathbf{1}_{Y\leq c}]-\mathbb{E}[Y\mathbf{1}_{Y\leq c}]=\mathbb{E}[X\mathbf{1}_{Y\leq c}]-\mathbb{E}[\mathbb{E}[X|Y]\mathbf{1}_{Y\leq c}].\]Since \[\{Y\leq c\}\in\sigma(Y)\] it follows that \[\mathbb{E}[\mathbb{E}[X|Y]\mathbf{1}_{Y\leq c}]=\mathbb{E}[X\mathbf{1}_{Y\leq c}]\] so \[\mathbb{E}[(X-Y)\mathbf{1}_{Y\leq c}]=0.\]
    \item[(b)] \begin{align*}
        \mathbb{E}[(X-Y)\mathbf{1}_{\{X\leq c\}\cap\{Y\leq c\}}]&=\mathbb{E}[(X-Y)\mathbf{1}_{\{Y\leq c\}}]-\mathbb{E}[(X-Y)\mathbf{1}_{\{X> c\}\cap\{Y\leq c\}}]\\&=-\mathbb{E}[(X-Y)\mathbf{1}_{\{X> c\}\cap\{Y\leq c\}}]\leq 0.
    \end{align*}
    \item[(c)] It follows from symmetry that \[\mathbb{E}[(Y-X)\mathbf{1}_{\{X\leq c\}\cap\{Y\leq c\}}]\leq0\] so \[\mathbb{E}[(X-Y)\mathbf{1}_{\{X\leq c\}\cap\{Y\leq c\}}]\geq 0\], and hence \[\mathbb{E}[(X-Y)\mathbf{1}_{\{X\leq c\}\cap\{Y\leq c\}}]=0.\]
    \item[(d)] \[\mathbb{E}[(X-Y)\mathbf{1}_{\{X\leq c\}\cap\{Y\leq c\}}]+\mathbb{E}[(X-Y)\mathbf{1}_{\{X>c\}\cap\{Y\leq c\}}]=\mathbb{E}[(X-Y)\mathbf{1}_{\{Y\leq c\}}]=0\]so\[\mathbb{E}[(X-Y)\mathbf{1}_{\{X>c\}\cap\{Y\leq c\}}]=0.\]Since\[(X-Y)\mathbf{1}_{\{X>c\}\cap\{Y\leq c\}}\geq0,\] it follows that \[(X-Y)\mathbf{1}_{\{X>c\}\cap\{Y\leq c\}}=0\] almost surely. That is,\[\mathbb{P}((X-Y)\mathbf{1}_{\{X>c\}\cap\{Y\leq c\}}>0)=\mathbb{P}(X>c,Y\leq c)=0.\]
    \item[(e)] \[\{X\neq Y\}=\bigcup_{c\in \mathbb{Q}}\{X>c,Y\leq c\}\cup\bigcup_{c\in\mathbb{Q}}\{Y>c,X\leq c\}.\]Hence,\[\mathbb{P}(X\neq Y)=\sum_{c\in\mathbb{Q}}\mathbb{P}(X>c,Y\leq c)+\sum_{c\in\mathbb{Q}}\mathbb{P}(Y>c,X\leq c)=0\] so\[X=Y\] almost surely.
\end{enumerate}
\end{proof}

\begin{exercise}
Assume that we toss $n$ fair coins and $N$ of them come up as heads. Suppose we then roll $N$ fair dice. Let $T$ be the sum of the values obtained on the $N$ dice. Express $\mathbb{E}[T|N]$ as a function of $N$, and then express $\mathbb{E}[T]$ and $\mathbb{E}[NT]$, both as functions of $n$.
\end{exercise}
\begin{proof}
\[\sigma(N)=\{\bigcup_{i\in I}\{N=i\}:I\subseteq\{0,...,n\},\] with \[\{N=0\},...,\{N=n\}\] being a partition of $\Omega$, and hence\begin{align*}\mathbb{E}[T|N]&=\sum_{i=0}^n\mathbb{E}[T|N=i]\mathbf{1}_{N=i}\\&=\sum_{i=0}^n3.5i\mathbf{1}_{N=i}\\&=3.5N.\end{align*}

Next,\[\mathbb{E}[T]=\mathbb{E}[T|N]=3.5\mathbb{E}[N]=3.5\cdot\frac{1}{2}n=1.75n.\]

Finally, \[\mathbb{E}[NT|N]=N\mathbb{E}[T|N]=3.5N^2.\] Hence\begin{align*}\mathbb{E}[NT]&=\mathbb{E}[\mathbb{E}[NT|N]]\\&=\mathbb{E}[3.5N^2]\\&=3.5\sum_{i=0}^ni^2\mathbb{P}(N=i)\\&=3.5\sum_{i=0}^ni^2{{n}\choose{i}}\frac{1}{2}^i\frac{1}{2}^{n-i}\\&=\frac{3.5}{2^n}\sum_{i=0}^ni^2{{n}\choose{i}}.\end{align*}
\end{proof}

\begin{exercise}
The aim of this question is to prove that, if \(X\) and \(Y\) are independent random variables and that both \(X\) and \(Y\) are in \(L^1\) (i.e. \(\mathbb{E}[|X|] < \infty\) and \(\mathbb{E}[|Y|] < \infty\)), then
\[
XY \in L^1 \quad \text{and} \quad \mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[Y]. \tag{1}
\]
In particular, if \(X\) and \(Y\) are independent random variables in \(L^2\), then
\[
\text{Cov}(X,Y) = 0 \quad \text{and} \quad \text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y).
\]

(Note that, without the independence assumption, in general \(X, Y \in L^1\) does \emph{not} imply \(XY \in L^1\).)

\begin{itemize}
    \item[(a)] Prove (1) under the assumption that \(X\) and \(Y\) are non-negative and bounded, i.e. there exists some \(K\) such that \(0 \leq X \leq K\) and \(0 \leq Y \leq K\) almost surely.
    \item[(b)] Prove (1) under the assumption that \(X\) and \(Y\) are non-negative. (Hint: set \(X_n = \min(X, n)\) and \(Y_n = \min(Y, n)\), for all \(n \geq 1\).)
    \item[(c)] Prove (1) in full generality.
\end{itemize}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] \[\mathbb{E}[XY]\leq\mathbb{E}[KY]=K\mathbb{E}[Y]<\infty\] so \[XY\in L^1.\]
    Hence,\[\mathbb{E}[XY]=\mathbb{E}[\mathbb{E}[XY|X]]=\mathbb{E}[X\mathbb{E}[Y|X]]=\mathbb{E}[X\mathbb{E}[Y]]=\mathbb{E}[X]\mathbb{E}[Y].\]
    \item[(b)] $X_n$ is $\sigma(X)$-measurable $\forall n\in\mathbb{N}$, since\[X_n=n\mathbf{1}_{\{X>n\}}+X\mathbf{1}_{\{X\leq n\}}.\] Hence \[\sigma(X_n)\subseteq\sigma(X)\forall n\]. Similarly, \[\sigma(Y_n)\subseteq\sigma(Y)\forall n\] and hence $X_n$ and $Y_n$ are independent $\forall n\in\mathbb{N}$. Hence, by (a), \[\mathbb{E}[X_nY_n]=\mathbb{E}[X_n]\mathbb{E}[Y_n]\forall n\in\mathbb{N}.\]
    Furthermore,
    \[0\leq X_n\uparrow X\text{ and }0\leq Y_n\uparrow Y\]so by the monotone convergence theorem
    \[\mathbb{E}[XY]=\mathbb{E}[\lim_{n\to\infty}X_nY_n]=\lim_{n\to\infty}\mathbb{E}[X_nY_n]=\lim_{n\to\infty}\mathbb{E}[X_n]\mathbb{E}[Y_n]=\mathbb{E}[X]\mathbb{E}[Y].\]
    \item[(c)] We have \[X=X^+-X^-,Y=Y^+-Y^-\] so \begin{align*}\mathbb{E}[XY]&=\mathbb{E}[(X^+-X^-)(Y^+-Y^-)]\\&=\mathbb{E}[X^+Y^+-X^+Y^--X^-Y^++X^-Y^-]\\&=\mathbb{E}[X^+]\mathbb{E}[Y^+]-\mathbb{E}[X^+]\mathbb{E}[Y^-]-\mathbb{E}[X^-]\mathbb{E}[Y^+]+\mathbb{E}[X^-]\mathbb{E}[Y^-]\\&=(\mathbb{E}[X^+]-\mathbb{E}[X^-])(\mathbb{E}[Y^+]-\mathbb{E}[Y^-])\\&=\mathbb{E}[X]\mathbb{E}[Y].\end{align*}
\end{enumerate}
\end{proof}

\begin{exercise}
Let $(\Omega, \mathcal{A}, \mathbb{P})$ be a probability space. Let $\mathcal{F} \subset \mathcal{G}$ be two sub-$\sigma$-algebras of $\mathcal{A}$, and $X$ a random variable on $(\Omega, \mathcal{A}, \mathbb{P})$.

\begin{itemize}
  \item[(a)] Prove that $\mathbb{E}[X \mathbb{E}[X \mid \mathcal{F}]] = \mathbb{E}[\mathbb{E}[X \mid \mathcal{F}]^2]$.
  
  \item[(b)] Prove that $\mathbb{E}[\mathbb{E}[X \mid \mathcal{F}] \mathbb{E}[X \mid \mathcal{G}]] = \mathbb{E}[\mathbb{E}[X \mid \mathcal{F}]^2]$.
  
  \item[(c)] Deduce from (i) and (ii) that
  \[
  \mathbb{E}[(X - \mathbb{E}[X \mid \mathcal{G}])^2] + \mathbb{E}[(\mathbb{E}[X \mid \mathcal{G}] - \mathbb{E}[X \mid \mathcal{F}])^2] = \mathbb{E}[(X - \mathbb{E}[X \mid \mathcal{F}])^2].
  \]
  
  \item[(d)] Recall that $\operatorname{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]$. Similarly, we define, for all sub-$\sigma$-algebras $\mathcal{F} \subset \mathcal{A}$,
  \[
  \operatorname{Var}(X \mid \mathcal{F}) := \mathbb{E}[(X - \mathbb{E}[X \mid \mathcal{F}])^2 \mid \mathcal{F}].
  \]
  Taking $\mathcal{F} = \{\emptyset, \Omega\}$ in (iii), show that, for any sub-$\sigma$-algebra $\mathcal{G} \subset \mathcal{A}$,
  \[
  \operatorname{Var}(X) = \mathbb{E}[\operatorname{Var}(X \mid \mathcal{G})] + \operatorname{Var}(\mathbb{E}[X \mid \mathcal{G}]).
  \tag{2}
  \]
  
  (Explain why $\mathcal{F} \subset \mathcal{G}$ for any sub-$\sigma$-algebra $\mathcal{G} \subset \mathcal{A}$.)
\end{itemize}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] \begin{align*}
        \mathbb{E}[X\mathbb{E}[X|\mathcal{F}]]&=\mathbb{E}[\mathbb{E}[X\mathbb{E}[X|\mathcal{F}]|\mathcal{F}]]\\&=\mathbb{E}[\mathbb{E}[X|\mathcal{F}]\mathbb{E}[X|\mathcal{F}]]\\&=\mathbb{E}[\mathbb{E}[X|\mathcal{F}]^2].
    \end{align*}
    \item[(b)] \begin{align*}
        \mathbb{E}[\mathbb{E}[X|\mathcal{F}]\mathbb{E}[X|\mathcal{G}]]&=\mathbb{E}[\mathbb{E}[\mathbb{E}[X|\mathcal{F}]X|\mathcal{G}]]\\&=\mathbb{E}[\mathbb{E}[X|\mathcal{F}]X]\\&=\mathbb{E}[\mathbb{E}[X|\mathcal{F}]^2].
    \end{align*}
    \item[(c)] \begin{align*}
        \mathbb{E}[(X - \mathbb{E}[X \mid \mathcal{G}])^2] + \mathbb{E}[(\mathbb{E}[X \mid \mathcal{G}] - \mathbb{E}[X \mid \mathcal{F}])^2]&=\mathbb{E}[X^2-2X\mathbb{E}[X|\mathcal{G}]+2\mathbb{E}[X|\mathcal{G}]^2-2\mathbb{E}[X|\mathcal{G}]\mathbb{E}[X|\mathcal{F}]+\mathbb{E}[X|\mathcal{F}]^2]\\&=\mathbb{E}[X^2-2\mathbb{E}[X|\mathcal{G}]^2+2\mathbb{E}[X|\mathcal{G}]^2-2\mathbb{E}[X|\mathcal{F}]^2+\mathbb{E}[X|\mathcal{F}]^2]\\&=\mathbb{E}[X^2-\mathbb{E}[X|\mathcal{F}]^2]\\&=\mathbb{E}[X^2-2X\mathbb{E}[X|\mathcal{F}]^2+\mathbb{E}[X|\mathcal{F}]^2]\\&=\mathbb{E}[(X-\mathbb{E}[X|\mathcal{F}])^2].
    \end{align*}
    \item[(d)] If $\mathcal{F}=\{\emptyset,\Omega\}$ then $\mathbb{E}[X|\mathcal{F}]=\mathbb{E}[X]$ almost surely. Hence\[\text{Var}(X)=\mathbb{E}[(X-\mathbb{E}[X|\mathcal{F}])^2].\] Furthermore,\[\mathbb{E}[(X-\mathbb{E}[X|\mathcal{G}])^2]=\mathbb{E}[\mathbb{E}[(X-\mathbb{E}[X|\mathcal{G}])^2]|\mathcal{G}]=\mathbb{E}[\text{Var}(X|\mathcal{G})]\] and\begin{align*}\mathbb{E}[(\mathbb{E}[X \mid \mathcal{G}] - \mathbb{E}[X \mid \mathcal{F}])^2]&=\mathbb{E}[(\mathbb{E}[X \mid \mathcal{G}] - \mathbb{E}[X ])^2]\\&=\mathbb{E}[(\mathbb{E}[X \mid \mathcal{G}] - \mathbb{E}[\mathbb{E}[X|\mathcal{G}]])^2]\\&=\text{Var}(\mathbb{E}[X|\mathcal{G}]).\end{align*}
    Hence \[\operatorname{Var}(X) = \mathbb{E}[\operatorname{Var}(X \mid \mathcal{G})] + \operatorname{Var}(\mathbb{E}[X \mid \mathcal{G}]).\]
\end{enumerate}
\end{proof}

\begin{exercise}
\textbf{Martingales associated with the simple random walk.}  
Let $p \in (0,1)$ and $(S_n)_{n \geq 1}$ be the simple random walk with parameter $p$ starting at $0$, i.e.
\[
S_n = \sum_{i=1}^n X_i,
\]
where $(X_i)_{i \geq 1}$ is a sequence of independent random variables satisfying  
\[
\mathbb{P}(X_i = 1) = p \quad \text{and} \quad \mathbb{P}(X_i = -1) = 1 - p \quad (\forall i \geq 1).
\]
Let $(\mathcal{F}_n)_{n \geq 1}$ be the natural filtration of $(X_i)_{i \geq 1}$.  
For all $n \geq 0$, define
\[
M_n := S_n - n\alpha, \quad M'_n := \beta^{S_n}, \quad \text{and} \quad M''_n := S_n^2 - n.
\]
\begin{enumerate}
\item[(a)] Find $\alpha$ and $\beta$ such that $(M_n)_{n \geq 1}$ and $(M'_n)_{n \geq 1}$ are martingales with respect to $(\mathcal{F}_n)_{n \geq 1}$.

\item[(b)] Show that if $p = 1/2$, then $(M''_n)_{n \geq 1}$ is a martingale with respect to $(\mathcal{F}_n)_{n \geq 1}$.
\end{enumerate}
\end{exercise}
\begin{proof}
Given any $\alpha$, $M_n$ is $\mathcal{F}_n$-measurable for all $n$. Furthermore, \[\mathbb{E}[|M_n|]=\mathbb{E}[|S_n-n\alpha|]\leq\mathbb{E}[|S_n|]+n|\alpha|\leq n+n|\alpha|<\infty,\] so $M_n$ is integrable for every $n$ and $\alpha$. We now need\[\mathbb{E}[M_{n+1}|\mathcal{F}_n]=M_n\text{ almost surely.}\]\begin{align*}\mathbb{E}[M_{n+1}|\mathcal{F}_n]&=\mathbb{E}[S_{n+1}-(n+1)\alpha|\mathcal{F}_n]\\&=\mathbb{E}\left[X_{n+1}+S_n-(n+1)\alpha\mid\mathcal{F}_n\right]\\&=\mathbb{E}[X_{n+1}
|\mathcal{F}_n]+S_n-(n+1)\alpha\\&=\mathbb{E}[X_{n+1}]+S_n-(n+1)\alpha\\&=2p-1+S_n-(n+1)\alpha\\&=M_n+2p-1-\alpha\text{ almost surely}.\end{align*}Hence $M_n$ is a martingale when \[2p-1-\alpha=0\iff \alpha=2p-1.\]

\noindent$f:\mathbb{R}\to\mathbb{R}:x\mapsto\beta^x$ is continuous $\forall \beta$ so $M_n'$ is $\mathcal{F}_n$-measurable $\forall n,\beta$.\[\mathbb{E}[|\beta^{S_n}|]\leq\mathbb{E}[|\beta^n|]<\infty\] so $M_n'$ is integrable.\begin{align*}
\mathbb{E}[M_{n+1}'|\mathcal{F}_n]&=\mathbb{E}[\beta^{S_{n+1}}|\mathcal{F}_n]\\&=\mathbb{E}[\beta^{X_{n+1}}\beta^{S_n}|\mathcal{F}_n]\\&=\beta^{S_n}\mathbb{E}[\beta^{X_{n+1}}|\mathcal{F}_n]\\&=M_n'\mathbb{E}[\beta^{X_{n+1}}]\\&=M_n'(p\beta+(1-p)\beta^{-1})\\&=M_n'.
\end{align*}
Hence, we need\begin{align*}
&p\beta+(1-p)\beta^{-1}=1\\\iff &p\beta^2-\beta+(1-p)=0\\\iff&\beta=\frac{1\pm\sqrt{1-4p(1-p)}}{2p}\\\iff&\beta=\frac{1\pm\sqrt{4p^2-4p+1}}{2p}\\\iff&\beta=\frac{1\pm|2p-1|}{2p}.
\end{align*} Hence either \[\beta=\frac{1+2p-1}{2p}=1\] or \[\beta=\frac{1-2p+1}{2p}=\frac{1-p}{p}.\]
\item[(b)] $M_n''$ is integrable, since\[\mathbb{E}[|M_n''|]\leq\mathbb{E}[|S_n^2|]+n\leq\mathbb{E}[|n^2|]+n<\infty.\] $M_n''$ is also clearly $\mathcal{F}_n$-measurable. \begin{align*}
\mathbb{E}[M_{n+1}''|\mathcal{F}_n]&=\mathbb{E}[S_{n+1}^2-n-1|\mathcal{F}_n]\\&=\mathbb{E}[(S_n+X_{n+1})^2-n-1|\mathcal{F}_n]\\&=\mathbb{E}[S_n^2+2S_nX_{n+1}+X_{n+1}^2-n-1|\mathcal{F}_n]\\&=M_n''+\mathbb{E}[2S_nX_{n+1}+X_{n+1}^2-1|\mathcal{F}_n]\\&=M_n''+2S_n\mathbb{E}[X_{n+1}]+\mathbb{E}[X_{n+1}^2]-1\\&=M_n''+0+\frac{1}{2}\cdot 1+\frac{1}{2}\cdot(-1)^2-1\\&=M_n''.
\end{align*}
Hence $M_n''$ is a martingale.
\end{proof}

\begin{exercise}
\textbf{Martingales associated with a Galton-Watson branching process.} Assume that $(Z_n)_{n\geq0}$ is the Galton-Watson branching process of offspring distribution $(p_i)_{i\geq0}$. Assume that $\mu=\sum_{i=1}^\infty ip_i\in(0,\infty)$. Let $(\mathcal{F}_n)_{n\geq1}$ be the natural filtration of $(Z_n)_{n\geq0}$.
\begin{enumerate}
    \item[(a)] For all $n\geq0$, set $M_n=\frac{Z_n}{\mu^n}$. Show that $(M_n)_{n\geq0}$ is a martingale with respect to $(\mathcal{F}_n)_{n\geq0}$.
    \item[(b)] Assume that $\alpha>0$ is such that $\sum_{i=0}^\infty\alpha^ip_i=\alpha$. For all $n\geq1$, set $M_n'=\alpha^{Z_n}$. Show that $(M_n')_{n\geq1}$ is a martingale with respect to $(\mathcal{F}_n)_{n\geq1}$.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
    \item[(a)] We have that \[\mathbb{E}[Z_{n+1}|\mathcal{F}_n]=\mu Z_n\forall n\geq 0\] so \[\mathbb{E}\left[\frac{Z_{n+1}}{\mu^{n+1}}|\mathcal{F}_n\right]=\frac{\mu Z_n}{\mu^{n+1}}=\frac{Z_n}{\mu^n}\] and hence $\frac{Z_n}{\mu^n}$ is a martingale with respect to $(\mathcal{F}_n)_{n\geq0}$.
    \item[(b)] First note that \[\mathbb{E}[\alpha^{\zeta_{n,i}}]=\sum_{i=0}^\infty\alpha^ip_i=\alpha.\]$M_n'$ is $\mathcal{F}_n$-measurable, since $f:\mathbb{R}\to\mathbb{R}:x\mapsto\alpha^x$ is continuous.
    
    We now show that $M_n'$ is integrable $\forall n$. Clearly $M_1'=\alpha^{\zeta_{0,1}}$ is integrable. Now assume $M_n'$ is integrable. We then have
    \begin{align*}\mathbb{E}[|M_{n+1}'|]&=\mathbb{E}[\alpha^{Z_{n+1}}]\\&=\mathbb{E}[\alpha^{\sum_{i=1}^{Z_n}\zeta_{n,i}}]\\&=\mathbb{E}[\sum_{k=0}^\infty\mathbf{1}_{\{Z_n=k\}}\alpha^{\sum_{i=1}^k\zeta_{n,i}}]\\&=\sum_{k=0}^\infty\mathbb{E}[\mathbf{1}_{\{Z_n=k\}}\alpha^{\sum_{i=1}^k\zeta_{n,i}}].\end{align*} For each $k$, we have\[\alpha^{\sum_{i=1}^k\zeta_{n,i}}=\prod_{i=1}^k\alpha^{\zeta_{n,i}}\in L^1,\] since $\alpha^{\zeta_{n,i}}\in L^1\forall n,i$. Hence, by independence,\begin{align*}
    \sum_{k=0}^\infty\mathbb{E}[\mathbf{1}_{\{Z_n=k\}}\alpha^{\sum_{i=1}^k\zeta_{n,i}}]&=\sum_{k=0}^\infty\mathbb{E}[\mathbf{1}_{\{Z_n=k\}}]\mathbb{E}[\alpha^{\sum_{i=1}^k\zeta_{n,i}}]\\&=\sum_{k=0}^\infty\mathbb{P}(Z_n=k)\alpha^k\\&=\mathbb{E}[\alpha^{Z_n}]<\infty.
    \end{align*} Hence $M_n'$ is integrable $\forall n$ by induction.
    Finally,\begin{align*}\mathbb{E}[M_{n+1}'|\mathcal{F}_n]&=\mathbb{E}[\alpha^{Z_{n+1}}|\mathcal{F}_n]\\&=\mathbb{E}[\alpha^{\sum_{i\geq1}\zeta_{n,i}\mathbf{1}_{\{i\leq Z_n\}}}|\mathcal{F}_n]\\&=\mathbb{E}\left[\sum_{k=0}^\infty\mathbf{1}_{\{Z_n=k\}}\alpha^{\sum_{i=1}^k\zeta_{n,i}}|\mathcal{F}_n\right]\\&=\sum_{k=0}^\infty\mathbb{E}[\mathbf{1}_{\{Z_n=k\}}\alpha^{\sum_{i=1}^k\zeta_{n,i}}|\mathcal{F}_n]\\&=\sum_{k=0}^\infty\mathbf{1}_{\{Z_n=k\}}\mathbb{E}[\alpha^{\sum_{i=1}^k\zeta_{n,i}}|\mathcal{F}_n]\\&=\sum_{k=0}^\infty\mathbf{1}_{\{Z_n=k\}}\mathbb{E}[\prod_{i=1}^k\alpha^{\zeta_{n,i}}]\\&=\sum_{k=0}^\infty\mathbf{1}_{\{Z_n=k\}}\alpha^k\\&=\alpha^{Z_n}\\&=M_n'\text{ almost surely.}\end{align*}Hence $M_n'$ is a martingale with respect to $(\mathcal{F}_n)_{n\geq1}$.
\end{enumerate}
\end{proof}

\begin{exercise}[Generalised PÃ³lya urn]
An urn initially contains $X_0 = x > 0$ grams of red powder and $Y_0 = y > 0$ grams of green powder.  Sample $(V_i)_{i\ge1}$, a sequence of non-negative i.i.d.\ random variables.  At each time step $n+1$ ($n\ge0$), we pick a speck of powder uniformly at random from the urn and then return it to the urn together with $V_{n+1}$ grams of powder of the same colour as that speck.

More formally, let $(U_i)_{i\ge1}$ be a sequence of i.i.d.\ $\mathrm{Unif}[0,1]$ random variables, independent of $(V_i)_{i\ge1}$.  Define $(X_n)_{n\ge0}$ and $(Y_n)_{n\ge0}$ by
\[
  (X_{n+1},Y_{n+1})
  = 
  \begin{cases}
    (\,X_n + V_{n+1},\,Y_n\,), 
      &\text{if } U_{n+1} < \dfrac{X_n}{X_n + Y_n}, \\[1em]
    (\,X_n,\,Y_n + V_{n+1}), 
      &\text{if } U_{n+1} \ge \dfrac{X_n}{X_n + Y_n}.
  \end{cases}
\]
For all $n\ge0$, set
\[
  M_n \;=\; \frac{X_n}{X_n + Y_n}.
\]
Show that $(M_n)_{n\ge0}$ is a martingale with respect to the filtration
\[
  \mathcal{F}_n \;=\;\sigma\bigl(U_1,V_1,\dots,U_n,V_n\bigr).
\]
\emph{(Hint: first consider the case where the $V_i$ are non-random.)}
\end{exercise}

\begin{proof}
We show that $X_n$ and $Y_n$ are $\mathcal{F}_n$-measurable by induction. For $n=0$, $X_0=x$ and $Y_0=y$ so $\sigma(X_0,Y_0)=\{\emptyset,\Omega\}\subseteq\mathcal{F}_0$. Now assume that $X_k$ and $Y_k$ are $\mathcal{F}_k$-measurable. Then \[X_{k+1}=X_k+V_{k+1}\mathbf{1}_{\{U_{k+1}<\frac{X_k}{X_k+Y_k}\}}.\] $X_k$ is $\mathcal{F}_{k+1}$-measurable by the inductive hypothesis, $V_{k+1}$ is $\mathcal{F}_{k+1}$-measurable by definition, and $\mathbf{1}_{\{U_{k+1}<\frac{X_k}{X_k+Y_k}\}}$ is $\mathcal{F}_{k+1}$ measurable since $\{U_{k+1}<\frac{X_k}{X_k+Y_k}\}\in\mathcal{F}_{k+1}$. Hence $X_{k+1}$ is $\mathcal{F}_{k+1}$ measurable. Similarly $Y_{k+1}$ is $\mathcal{F}_{k+1}$-measurable. Hence $X_n$ and $Y_n$ are both $\mathcal{F}_n$-measurable for all $n$ by induction. hence, $M_n$ is $\mathcal{F}_{n}$-measurable $\forall n$.

We now show that $M_n$ is integrable $\forall n$. Clearly $M_0$ is integrable. Otherwise,\[M_{n+1}=\frac{X_n+V_{n+1}\mathbf{1}_{\{U_{n+1}<\frac{X_n}{X_n+Y_n}\}}}{X_n+Y_n+V_{n+1}}<1.\] Hence $M_n$ is integrable $\forall n$.

\begin{align*}\mathbb{E}[M_{n+1}|\mathcal{F}_n]&=\mathbb{E}\left[\frac{X_n+V_{n+1}\mathbf{1}_{\{U_{n+1}<\frac{X_n}{X_n+Y_n}\}}}{X_n+Y_n+V_{n+1}}|\mathcal{F}_n\right]\\&=\mathbb{E}\left[\sum_{i=0}^\infty\mathbf{1}_{\{V_{n+1}=i\}}\frac{X_n+i\mathbf{1}_{\{U_{n+1}<\frac{X_n}{X_n+Y_n}\}}}{X_n+Y_n+i}|\mathcal{F}_n\right]\\&=\sum_{i=0}^\infty\mathbb{E}\left[\mathbf{1}_{\{V_{n+1}=i\}}\frac{X_n+i\mathbf{1}_{\{U_{n+1}<\frac{X_n}{X_n+Y_n}\}}}{X_n+Y_n+i}|\mathcal{F}_n\right]\\&=\sum_{i=0}^\infty\frac{\mathbb{E}[\mathbf{1}_{\{V_{n+1}=i\}}(X_n+i\mathbf{1}_{\{U_{n+1}<\frac{X_n}{X_n+Y_n}\}})|\mathcal{F}_n]}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\frac{X_n\mathbb{E}[\mathbf{1}_{\{V_{n+1}=i\}}|\mathcal{F}_n]+\mathbb{E}[i\mathbf{1}_{\{V_{n+1}=i\}}\mathbf{1}_{\{U_{n+1}<\frac{X_n}{X_n+Y_n}\}}|\mathcal{F}_n]}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\frac{X_n\mathbb{P}(V_{n+1}=i)+i\mathbb{E}[\mathbf{1}_{\{V_{n+1}=i\}}\mathbf{1}_{\{U_{n+1}<\frac{X_n}{X_n+Y_n}\}}|\mathcal{F}_n]}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\frac{X_n\mathbb{P}(V_{n+1}=i)+i\mathbb{P}(V_{n+1}=i,U_{n+1}<\frac{X_n}{X_n+Y_n}|\mathcal{F}_n)}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\frac{X_n\mathbb{P}(V_{n+1}=i)+i\mathbb{P}(V_{n+1}=i)M_n}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\mathbb{P}(V_{n+1}=i)\frac{X_n+iM_n}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\mathbb{P}(V_{n+1}=i)\frac{X_n+\frac{iX_n}{X_n+Y_n}}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\mathbb{P}(V_{n+1}=i)\frac{\frac{X_n(X_n+Y_n+i)}{X_n+Y_n}}{X_n+Y_n+i}\\&=\sum_{i=0}^\infty\mathbb{P}(V_{n+1}=i)M_n\\&=M_n\text{ almost surely.}\end{align*}
Hence $M_n$ is a martingale with respect to $(\mathcal{F}_n)_{n\geq0}$.
\end{proof}

\begin{exercise}
\begin{enumerate}
    \item[(a)] Assume that $(X_i)_{i \geq 1}$ is a sequence of random variables and 
    $\mathcal{F}_n = \sigma(X_1, X_2, \ldots, X_n)$ for all $n \geq 1$. 
    Which of the following random variables are stopping times 
    (use the convention $\min \emptyset = +\infty$)? Justify your answers.
    
    \begin{enumerate}
        \item[i.] $T_1 = \min\{n \geq 1 : X_n \in [15, 20]\}$
        \item[ii.] $T_2 = \min\{n \geq 1 : X_1 + 2X_2 + \cdots + nX_n \geq 120\}$
        \item[iii.] $T_3 = 5$
        \item[iv.] $T_4 = \min\{n \geq 2 : X_n = 8 \text{ and } X_{n-1} = 6\}$
        \item[v.] $T_5 = \min\{n \geq 1 : X_n = 8 \text{ and } X_{n+1} = 6\}$
    \end{enumerate}
    
    \item[(b)] Suppose $S$ and $T$ are stopping times. Show that $\min(S, T)$ and $S + T$ are stopping times.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
\item[(a)] 
\begin{itemize}
\item[(i)] \[\{T_1\leq n\}=\bigcup_{i=1}^n\{X_i\in[15,20]\}\in\mathcal {F}_n\] so $T_1$ is a stopping time.
\item[(ii)] \[\{T_2\leq n\}=\bigcup_{i=1}^n\left\{\sum_{j=1}^ijX_j\geq120\right\}\in\mathcal{F}_n\] so $T_2$ is a stopping time.
\item[(iii)] \[\{T_3=n\}\in\{\emptyset,\Omega\}\] so $T_3$ is trivially a stopping time.
\item[(iv)] \[\{T_4=1\}=\emptyset\in\mathcal{F}_1.\]And for $n\geq 2,$\[\{T_4\leq n\}=\bigcup_{i=2}^n\{X_i=8,X_{i-1}=6\}\in\mathcal{F}_n.\] Hence $X_4$ is a stopping time.
\item[(v)] \[\{T_5=1\}=\{X_1=8,X_2=6\}\not\in\mathcal{F}_1\] so $T_5$ is not a stopping time.
\end{itemize}
\item[(b)] \[\{\min(S,T)\leq n\}=\{S\leq n\}\cup\{T\leq n\}\in\mathcal{F}_n\] so $\min(S,T)$ is a stopping time.\[\{S+T=n\}=\bigcup_{i=1}^{n-1}\{S=i,T=n-i\}\in\mathcal{F}_n.\] Hence $S+T$ is a stopping time.
\end{enumerate}
\end{proof}

\begin{exercise}
An investor buys shares in a company. At the end of each six-month period, the shareholder will receive dividends of size $0$, $1$, or $2$ per share, with probability $1/6$, $1/2$, and $1/3$, respectively. Let $(X_i)_{i \geq 1}$ be the successive dividends (per share) received by the investor (assumed i.i.d.).

\medskip

The investor sells his shares at the first time when the dividend is of size $0$ (say, on the $T$-th dividend: $X_T = 0$). Let $S = \sum_{i=1}^{T} X_i$ be the total dividend income per share received by the investor before they sell. Calculate $\mathbb{E}[S]$.
\end{exercise}
\begin{proof}
\[T=\inf\{n\geq1:X_n=0\},\] which is clearly a stopping time. Furthermore, \[\mathbb{E}[T]=\sum_{n=1}^\infty\mathbb{P}(T\geq n)=\sum_{n=1}^\infty\left(\frac{5}{6}\right)^{n-1}=6<\infty\] and\[\mathbb{E}[X_1]=\frac{1}{2}+\frac{2}{3}=\frac{7}{6}<\infty.\] Hence Wald's equation applies to give\[\mathbb{E}[S]=\frac{7}{6}\cdot 6=7.\]
\end{proof}

\begin{exercise}
Let $(S_n)_{n \geq 0}$ be the simple random walk with parameter $p \neq 1/2$. Let $a > 0$ and $b > 0$ be integers. Let 
\[
T = \min\{n \geq 1 : S_n = a \text{ or } S_n = -b\}.
\]
Find a formula for $\mathbb{E}[T]$.

\medskip

\textit{You can use without a proof that $\mathbb{E}[T] < +\infty$. $\mathbb{P}(S_T = a) = \dfrac{\alpha^b - 1}{\alpha^{a+b} - 1}$, where $\alpha = \dfrac{1 - p}{p}$.}

\textit{(See Equation (8.5) in the lecture notes for the second claim. The first claim can be proved by adapting the proof of $\mathbb{E}[T] < +\infty$ in the symmetric case in Section 7.2.)}
\end{exercise}
\begin{proof}
By Wald's equation,\[\mathbb{E}[S_T]=\mathbb{E}[X_1]\mathbb{E}[T]=(2p-1)\mathbb{E}[T].\] Furthermore,\[\mathbb{E}[S_T]=a\mathbb{P}(S_T=a)-b\mathbb{P}(S_T=-b)=a\frac{\alpha^b-1}{\alpha^{a+b}-1}-b\left(1-\frac{\alpha^b-1}{\alpha^{a+b}-1}\right).\]Hence,\[\mathbb{E}[T]=\frac{1}{2p-1}\left(a\frac{\alpha^b-1}{\alpha^{a+b}-1}-b\left(1-\frac{\alpha^b-1}{\alpha^{a+b}-1}\right)\right).\]
\end{proof}

\begin{exercise}
The transmitter spaceship "Supermartingale" was struck by a meteorite and has started sending out random letters into space. Assume that the letters $X_1,X_2,...$ sent after the strike are i.i.d., with each $X_n$ uniformly distributed over the $26$ letters of the alphabet. Let $T$ denote the first time the transmitter sends the crucial sequence "SOS", i.e.\[T=\inf\{n\geq3:X_n=\text{S},X_{n-1}=\text{O},X_{n-2}=\text{S}\},\]so that $\{T\leq2\}=\emptyset$. Let $V_0=0$ and, for all $i\geq1$, let $V_i=\min\{n>V_{i-1}:X_n=\text{S}\}$. Also set $W_i=V_i-V_{i-1}$ for all $i\geq1$. Finally, for all $i\geq1$, let $Y_i$ denote the block of letters $(X_{V_{i-1}+1},...,X_{V_i})$, and set $T'=\min\{V_i:Y_i=\text{OS}\}$ be the number of letters up to the end of the first block to take the value OS.
\begin{enumerate}
\item[(a)] Find $\mathbb{E}[W_1]$ and find $\mathbb{P}(Y_1=\text{OS})$.
\item[(b)] Let $N$ be the total number of times the letter S is emitted up to and including time $T'$. Find $\mathbb{E}[N]$, justifying your answer briefly.
\item[(c)] Calculate $\mathbb{E}[T']$.
\item[(d)] Calculate $\mathbb{E}[T]$.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
\item[(a)] \[\mathbb{E}[W_1]=\sum_{n=1}^\infty\mathbb{P}(W_1\geq n)=\sum_{n=1}^\infty\left(\frac{25}{26}\right)^{n-1}=26.\]\[\mathbb{P}(Y_1=\text{OS})=\left(\frac{1}{26}\right)^2.\]
\item[(b)]
\begin{align*}\mathbb{P}(N> n)&=\mathbb{P}(N> n|N> n-1)\mathbb{P}(N> n-1)\\&=(1-\mathbb{P}(Y_n=\text{OS}))\mathbb{P}(N> n-1)\\&=(1-\mathbb{P}(Y_1=\text{OS}))\mathbb{P}(N>n-1)\\&=\frac{675}{676}\mathbb{P}(N>n-1).\end{align*}
Hence by induction\[\mathbb{P}(N>n)=\left(\frac{675}{676}\right)^n\]so\[\mathbb{E}[N]=\sum_{n=1}^\infty\mathbb{P}(N>n-1)=\sum_{n=1}^\infty\left(\frac{675}{676}\right)^{n-1}=676.\]
\item[(c)] \[T'=V_N=\sum_{i=1}^NW_i.\] $W_1,W_2,...$ are clearly i.i.d., so Wald's equation applies to give \[\mathbb{E}[T']=\mathbb{E}[W_1]\mathbb{E}[N]=26\cdot 676=26^3.\]
\item[(d)]
\begin{align*}
\mathbb{E}[T|N]&=\mathbb{E}[T\mathbf{1}_{\{N>1\}}+T\mathbf{1}_{\{N=1\}}|N]\\&=\mathbb{E}[T'\mathbf{1}_{\{N>1\}}+T\mathbf{1}_{\{N=1\}}|N]\\&=\mathbf{1}_{\{N>1\}}\mathbb{E}[T'|N]+\mathbf{1}_{\{N=1\}}\mathbb{E}[T|N=1]\\&=\mathbf{1}_{\{N>1\}}\mathbb{E}[T'|N]+\mathbf{1}_{\{N=1\}}(\mathbb{E}[T']+2)
\end{align*}
so\begin{align*}
\mathbb{E}[T]&=\mathbb{E}[\mathbf{1}_{\{N>1\}}\mathbb{E}[T'|N]]+(\mathbb{E}[T']+2)\mathbb{P}(N=1)\\&=\mathbb{E}[T'\mathbf{1}_{\{N>1\}}]+(\mathbb{E}[T']+2)\mathbb{P}(Y_1=\text{OS})\\&=\mathbb{E}[T']-\mathbb{E}[T'\mathbf{1}_{\{N=1\}}]+(\mathbb{E}[T']+2)\mathbb{P}(Y_1=\text{OS})\\&=\mathbb{E}[T']+(\mathbb{E}[T']+2)\mathbb{P}(Y_1=\text{OS})-\mathbb{E}[T'|N=1]\mathbb{P}(Y_1=\text{OS})\\&=\mathbb{E}[T']+(\mathbb{E}[T']+2)\mathbb{P}(Y_1=\text{OS})-2\mathbb{P}(Y_1=\text{OS})\\&=26^3+\frac{(26^3+2)}{26^2}-\frac{2}{26^2}=17602.
\end{align*}
\end{enumerate}
\end{proof}

\begin{exercise}
Suppose you bet on successive tosses of a fair coin. You bet an amount $2^{n-1}$ on the $n$-th toss of the coin, so that, at the $n$-th toss you win or lose $2^{n-1}$ with equal probability. For all $n \geq 0$, let $M_n$ be your total winnings after $n$ tosses ($M_0 = 0$).

\begin{enumerate}
    \item[(i)] Show that $(M_n)_{n \geq 0}$ is a martingale with respect to an appropriate filtration.
    
    \item[(ii)] Let $T$ be the first time you win the bet; assume that you stop betting at time $T$. Show that $M_T = 1$ with probability $1$, and deduce that the conclusion of the Optional Stopping Theorem fails. Which of the conditions of the OST fail?
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
\item[(i)] Let $(X_i)_{i\geq0}$ be a sequence of i.i.d. random variables such that \[\mathbb{P}(X_1=1)=\mathbb{P}(X_1=-1)=\frac{1}{2}.\]Then \[M_n=\sum_{i=1}^n2^{i-1}X_i\forall n.\] let $(\mathcal{F}_n)_{n\geq0}$ be the natural filtration of $(X_i)_{i\geq0}$.
$M_n$ is clearly $\mathcal{F}_n$-measurable $\forall n$. Furthermore,\[\mathbb{E}[|M_n|]=\mathbb{E}\left[\left|\sum_{i=1}^n2^{i-1}X_i\right|\right]\leq\left[\sum_{i=1}^n2^{i-1}\right]<\infty.\]
Finally,\begin{align*}\mathbb{E}[M_{n+1}|\mathcal{F}_n]&=\mathbb{E}[2^nX_{n+1}+M_n|\mathcal{F}_n]\\&=2^n\mathbb{E}[X_{n+1}|\mathcal{F}_n]+M_n\\&=2^n\mathbb{E}[X_{n+1}]+M_n\\&=M_n\text{ almost surely.}\end{align*} Hence $(M_n)_{n\geq0}$ is a martingale with respect to $(\mathcal{F}_n)_{n\geq0}$.
\item[(ii)] \[M_T=2^{T-1}-\sum_{i=1}^{T-1}2^{i-1}=1.\] This is different to $\mathbb{E}[M_0]=0$. $T$ is clearly a stopping time with respect to $(\mathcal{F}_n)_{n\geq0}$. Furthermore, \[\mathbb{P}(T=\infty)=\lim_{n\to\infty}\left(\frac{1}{2}\right)^n=0\] so $T<\infty$ almost surely.\[\mathbb{E}[|M_T|]=1<\infty\] so in order for the Optional stopping theorem to fail we need\[\lim_{n\to\infty}\mathbb{E}[M_n\mathbf{1}_{\{T>n\}}]\neq0.\]
\end{enumerate}
\end{proof}

\begin{exercise}
At time 0, a bag contains one red marble and one green marble. At each time step $1, 2, \dots$, we draw a ball uniformly at random from the bag, and then replace it in the bag together with an additional marble of the same colour. Let $R_n$ denote the number of red balls in the bag after $n$ steps ($R_0 = 1$). Recall that $M_n = \frac{R_n}{n+2}$ is a martingale with respect to $(\mathcal{F}_n)_{n \geq 0}$ with $\mathcal{F}_n = \sigma(R_1, \dots, R_n)$, for all $n \geq 1$, and $\mathcal{F}_0 = \{\emptyset, \Omega\}$. Let $T$ be the number of marbles drawn until the first draw of a red marble.

\begin{enumerate}
    \item[(i)] Show that $T$ is an $(\mathcal{F}_n)_{n \geq 0}$-stopping time.
    
    \item[(ii)] Show that $T < \infty$ almost surely.
    
    \item[(iii)] Show that
    \[
    \mathbb{E} \left[ \frac{1}{T + 2} \right] = \frac{1}{4}.
    \]
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
\item[(i)] \[\{T>n\}=\bigcap_{i=0}^n\{R_i=1\}\in\mathcal{F}_n\forall n\] so $T$ is an $(\mathcal{F}_n)_{n \geq 0}$-stopping time.
\item[(ii)] \[\mathbb{P}(T=\infty)=\lim_{n\to\infty}\prod_{i=1}^n\frac{i}{(i+1)}=\lim_{n\to\infty}\frac{1}{n+1}=0\] so $T<\infty$ almost surely.
\item[(iii)] $M_n$ is bounded so the Optional stopping theorem applies, to give \[\mathbb{E}[M_T]=\mathbb{E}[M_0].\]Furthermore,\[M_T=\frac{2}{T+2}\] and\[\mathbb{E}[M_0]=\frac{1}{2}\] so\[\mathbb{E}\left[\frac{1}{T+2}\right]=\frac{1}{4}.\]
\end{enumerate}
\end{proof}

\begin{exercise}
For all $n \geq 0$, let $Y_n$ be the assets (in GBP) of an insurance company after $n$ years of trading. We assume that $Y_0 \in (0, \infty)$ is deterministic. Each year, the company receives a total income of $P$ (a fixed amount) in premiums, and pays out a total of $C_n$, so
\[
Y_{n+1} = Y_n + P - C_{n+1} \quad \text{for all } n \geq 0.
\]
Assume that $(C_i)_{i \geq 1}$ is a sequence of i.i.d. $\mathcal{N}(\mu, \sigma^2)$ random variables with $\mu < P$, and therefore have moment generating function
\[
\mathbb{E}[e^{\theta C_1}] = \exp(\mu \theta + \sigma^2 \theta^2 / 2). \tag{1}
\]

\begin{enumerate}
    \item[(i)] Show that $M_n := e^{-\theta Y_n}$ is a martingale for a certain choice of $\theta > 0$.
    
    \item[(ii)] We say that the company becomes bankrupt if there exists $n \geq 1$ such that $Y_n < 0$. Using the Elementary Stopping Lemma, show that
    \[
    \mathbb{P}(Y_n < 0 \text{ for some } n) \leq \exp(-2(P - \mu)Y_0 / \sigma^2).
    \]
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
\item[(i)] Let $(\mathcal{F}_n)_{n\geq0}$ be the natural filtration of $(Y_n)_{n\geq0}$. Then $M_n$ is clearly $\mathcal{F}_n$-measurable $\forall n$.\[Y_n=Y_0+nP-\sum_{i=1}^nC_i\]so\[M_n=e^{-\theta(Y_0+nP-\sum_{i=1}^nC_i)}=e^{-\theta Y_0}\prod_{i=1}^ne^{\theta(C_i-P)}.\] If \[\mathbb{E}[e^{\theta(C_i-P)}]=1\forall i\] then $M_n$ is a martingale. Hence we need\begin{align*}
\exp(\mu\theta+\sigma^2\theta^2/2)&=\exp(\theta P)\\\iff\mu\theta+\sigma^2\theta^2/2&=\theta P\\\iff\mu+\sigma^2\theta/2&=P\\\iff\theta&=\frac{2}{\sigma^2}(P-\mu).
\end{align*}
\end{enumerate}
\item[(ii)] Let $T:=\inf\{n\geq1:Y_n<0\}$. $T$ is an $(\mathcal{F}_n)_{n\geq0}$-stopping time, so by the Elementary stopping lemma\[\mathbb{E}[M_{n\wedge T}]=\mathbb{E}[M_0]=\exp(-\theta Y_0)\forall n\geq0.\]Furthermore,\[Y_n<0\iff M_n>1\]so\[\mathbb{E}[M_{n\wedge T}]\geq\mathbb{E}[M_{n\wedge T}\mathbf{1}_{\{T\leq n\}}]=\mathbb{E}[M_T\mathbf{1}_{\{T\leq n\}}]\geq\mathbb{P}(T\leq n)\forall n.\]Hence\[\mathbb{P}(T<\infty)\leq\exp\left(-\frac{2}{\sigma^2}(P-\mu)Y_0\right).\]
\end{proof}

\begin{exercise}
Let $p \in (0,1) \setminus \{\tfrac{1}{2}\}$ and $a, b$ be two integers such that $1 \leq a < b$. Let $(X_i)_{i \geq 1}$ be a sequence of i.i.d.\ random variables such that $\mathbb{P}(X_i = 1) = p$ and $\mathbb{P}(X_i = -1) = q = 1 - p$. Let $S_0 = a$, and $S_n = a + \sum_{i=1}^n X_i$. Finally, for all $n \geq 0$, set
\[
M_n = \left( \frac{q}{p} \right)^{S_n}.
\]

Recall that $(M_n)_{n \geq 0}$ is a martingale with respect to $(\mathcal{F}_n)_{n \geq 0}$, the natural filtration of $(S_n)_{n \geq 0}$. Recall that 
\[
T := \inf\{n \geq 0 : S_n = 0 \text{ or } S_n = b\}
\]
is the stopping time when $S$ first hits $0$ or $b$.

\begin{enumerate}
    \item[(a)] Show that $M_{T \wedge n}$ converges almost surely as $n \to +\infty$.
    \item[(b)] Reasoning by contradiction, deduce that $\mathbb{P}(T < \infty) = 1$.
    \item[(c)] Calculate $\mathbb{P}(S_T = 0)$.
    \item[(d)] Show that $\mathbb{E}[S_{T \wedge n}] \to \mathbb{E}[S_T]$ as $n \uparrow \infty$.
    \item[(e)] Show that, as $n \uparrow \infty$, $\mathbb{E}[T \wedge n] \to \mathbb{E}[T]$.
    \item[(f)] Find an expression for $\mathbb{E}[T]$.
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
\item[(a)] $M_{n\wedge T}$ is a martingale, and by the elementary stopping lemma\[\mathbb{E}[M_{n\wedge T}]=\mathbb{E}[M_0]=\left(\frac{q}{p}\right)^a\forall n.\] Hence \[\sup_{n\geq0}\mathbb{E}[|M_{n\wedge T}|]<\infty\] so by the martingale convergence theorem $M_{n\wedge T}$ converges almost surely.
\item[(b)] Suppose that $T=\infty$ and $M_{n\wedge T}$ converges for some $\omega\in\Omega$. If $T=\infty$ then $M_{n\wedge T}=M_n\forall n$ so \[\lim_{n\to\infty}M_{n\wedge T}=\lim_{n\to\infty}\left(\frac{q}{p}\right)^{S_n}.\] However, we also have\[0<S_n<b\forall n\] so $M_n$ takes finitely many values. There must be at least two values which $M_n$ can take, since otherwise $T$ would occur. Let $\epsilon$ be a quarter of the smallest difference between any two values which $M_n$ takes. There then does not exist an $N\in\mathbb{N}$ such that $|M_n-M_\infty|<\epsilon\forall n>N$, which is a contradiction; hence the limit does not exist. Hence \[\{T=\infty\}\cap\{M_{n\wedge T}\text{ converges}\}=\emptyset.\] That is, for every $\omega\in\Omega$ such that $M_{n\wedge T}$ converges, $T<\infty$. Hence\[\mathbb{P}(T=\infty)\leq\mathbb{P}(M_{n\wedge T}\text{ does not converge})=0\] so\[\mathbb{P}(T<\infty)=1.\]
\item[(c)] The probability that $S_T=0$ is the same as the probability that we hit $-a$ before $b-a$ for a random walk starting at $0$. Hence\begin{align*}
\mathbb{P}(S_T=0)&=1-\frac{\alpha^{a}-1}{\alpha^{b-a+a}-1}\\&=1-\frac{\alpha^a-1}{\alpha^b-1}\\&=\frac{\alpha^b-\alpha^a}{\alpha^b-1}
\end{align*}where \[\alpha:=\frac{q}{p}.\]
\item[(d)] $|S_{n\wedge T}|\leq b\forall n$ almost surely. Furthermore, $T<\infty$ almost surely, so there exists an $N\in\mathbb{N}$ such that $S_{n\wedge T}=S_T\forall n> N$ almost surely, and hence $S_{n\wedge T}\to S_T$ as $n\uparrow\infty$ almost surely. Hence by the dominated convergence theorem \[\mathbb{E}[S_{n\wedge T}]\to\mathbb{E}[S_T]\text{ as }n\uparrow\infty.\]
\item[(e)] $T$ is finite almost surely so $n\wedge T\uparrow T$ almost surely. Hence by the monotone convergence theorem\[\mathbb{E}[n\wedge T]\to\mathbb{E}[T]\text{ as }n\to\infty.\]
\item[(f)] \[\mathbb{E}[S_T]=b\mathbb{P}(S_T=b)=b\frac{\alpha^a-1}{\alpha^b-1}.\]$n\wedge T$ is a stopping time so by Wald's equation\[\mathbb{E}[S_{n\wedge T}]-a=(p-q)\mathbb{E}[n\wedge T]\] so\[\lim_{n\to\infty}\mathbb{E}[S_{n\wedge T}]=a+(p-q)\mathbb{E}[T].\] Furthermore,\[\lim_{n\to\infty}\mathbb{E}[S_{n\wedge T}]=\mathbb{E}[S_T]=b\frac{\alpha^a-1}{\alpha^b-1}\] and hence\[\mathbb{E}[T]=\frac{1}{p-q}\left(b\frac{\alpha^a-1}{\alpha^b-1}-a\right).\]
\end{enumerate}
\end{proof}


\begin{exercise}
\textit{Martingale formulation of Bellman's Optimality Principle.}
A player plays a game for $N$ units of time. At each time $n \geq 1$, the winnings per unit stake are $\varepsilon_n$, where $(\varepsilon_n)_{n \geq 1}$ is a sequence of i.i.d.\ random variables such that, for all $n \geq 1$,
\[
\mathbb{P}(\varepsilon_n = +1) = p, \quad \mathbb{P}(\varepsilon_n = -1) = q, \quad \text{where} \quad \frac{1}{2} < p = 1 - q < 1.
\]

For all $n \geq 0$, we let $Z_n$ denote the fortune of the player at time $n$. The playerâs stake at time $n$, $C_n$, must lie in $(0, Z_{n-1})$. The playerâs aim is to maximise the expected ``interest rate'' $\mathbb{E}[\log(Z_N / Z_0)]$. For all $n \geq 0$, we set 
\[
\mathcal{F}_n = \sigma(C_1, \varepsilon_1, \ldots, C_n, \varepsilon_n, C_{n+1}).
\]
Let
\[
\alpha := p \log p + q \log q + \log 2.
\]

You can use without proof that $\log Z_n$ is integrable for all $n \geq 0$.

\begin{enumerate}
    \item[(a)] Show that $(\log Z_n - n \alpha)_{n \geq 0}$ is a super-martingale with respect to $(\mathcal{F}_n)_{n \geq 0}$. \textit{(Hint: you may want to show that the function $x \mapsto p \log(1 + x) + q \log(1 - x)$ on $[0, 1)$ reaches its maximum at $x = p - q$.)}
    
    \item[(b)] Deduce that $\mathbb{E}[\log(Z_N / Z_0)] \leq N \alpha$.
    
    \item[(c)] Find a process $(C_n)_{n \geq 1}$ such that $(\log Z_n - n \alpha)_{n \geq 0}$ is an $(\mathcal{F}_n)_{n \geq 0}$-martingale. What is the best strategy for the player?
\end{enumerate}
\end{exercise}
\begin{proof}
\begin{enumerate}
\item[(a)] \[Z_n=Z_{n-1}+\epsilon_nC_n\forall n\geq1.\]\[Z_n=Z_0+\sum_{i=1}^n\epsilon_iC_i\forall n\]$Z_n$ is clearly $\mathcal{F}_n$-measurable, and hence so is $\log Z_n-n\alpha$, as the composition of $Z_n$ with a continuous function. Define $f:[0,1)\to\mathbb{R}:x\mapsto p\log(1+x)+q\log(1-x)$. $f'(x)=\frac{p}{1+x}-\frac{q}{1-x}$ is zero when \[p(1-x)=q(1+x)\iff (p+q)x=p-q\iff x=\frac{p-q}{p+q}=p-q.\] Furthermore,\[f'(0)=p-q>0\] so $x=0$ is not the maximum value of $f$, and \[f'(x)<0\iff p(1-x)<q(1+x)\iff (p+q)x>p-q\iff x>p-q.\] Hence $x=p-q$ is the maximum of $f$. Since \[1+(p-q)=1+p-(1-p)=2p\] and \[1-(p-q)=1-p+1-p=2q\] It follows that \begin{align*}p\log(2p)+q\log(2q)&=p\log 2+p\log p + q\log 2 +q\log q\\&=p\log p+q\log q+\log 2=\alpha\end{align*} is the maximum value of $f$.\begin{align*}
\mathbb{E}[\log Z_{n+1}-(n+1)\alpha|\mathcal{F}_n]&=\mathbb{E}[\log Z_{n+1}|\mathcal{F}_n]-(n+1)\alpha\\&=\mathbb{E}[\log(Z_n+\epsilon_{n+1}C_{n+1})|\mathcal{F}_n]-(n+1)\alpha\\&=\mathbb{E}\left[\log\left(Z_n\left(1+\frac{\epsilon_{n+1}C_{n+1}}{Z_n}\right)\right)|\mathcal{F}_n\right]-(n+1)\alpha\\&=\log Z_n+\mathbb{E}\left[\log\left(1+\frac{\epsilon_{n+1}C_{n+1}}{Z_n}\right)|\mathcal{F}_n\right]-(n+1)\alpha\\&=\log Z_n+p\log\left(1+\frac{C_{n+1}}{Z_n}\right)+q\log\left(1-\frac{C_{n+1}}{Z_n}\right)-(n+1)\alpha\\&\leq\log Z_n+\alpha-(n+1)\alpha\\&=\log Z_n-n\alpha.
\end{align*}Hence $\log Z_n-n\alpha$ is a super-martingale.
\item[(b)] \[\mathbb{E}[\log Z_{n+1}-(n+1)\alpha]=\mathbb{E}[\mathbb{E}[\log Z_{n+1}-(n+1)\alpha|\mathcal{F}_n]]\leq\mathbb{E}[\log Z_n-n\alpha]\forall n\] so\[\mathbb{E}[\log Z_{n+1}]\leq \mathbb{E}[\log Z_n]+\alpha\forall n.\] Hence\[\mathbb{E}[\log Z_n]\leq \mathbb{E}[\log Z_0]+N\alpha\] so\[\mathbb{E}[\log(Z_n/Z_0)]\leq N\alpha.\]
\item[(c)] On each turn, let \[C_{n+1}=(p-q)Z_n.\]
\end{enumerate}
\end{proof}

\end{document}
